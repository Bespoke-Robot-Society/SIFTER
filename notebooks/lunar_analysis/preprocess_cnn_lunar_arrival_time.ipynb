{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read\n",
    "from scipy import signal\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Set device (CPU or GPU)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SMOTE.__init__() got an unexpected keyword argument 'n_neighbors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 254\u001b[0m\n\u001b[1;32m    251\u001b[0m     evaluate_model(cnn_model, test_loader)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 254\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 236\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Oversample minority classes in the training set\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m X_train_res, y_train_res \u001b[38;5;241m=\u001b[39m \u001b[43moversample_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of training data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_res\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Prepare data for PyTorch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 217\u001b[0m, in \u001b[0;36moversample_data\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moversample_data\u001b[39m(X_train, y_train):\n\u001b[0;32m--> 217\u001b[0m     sm \u001b[38;5;241m=\u001b[39m \u001b[43mSMOTE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Set n_neighbors to 1\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     X_res, y_res \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train)\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_res, y_res\n",
      "\u001b[0;31mTypeError\u001b[0m: SMOTE.__init__() got an unexpected keyword argument 'n_neighbors'"
     ]
    }
   ],
   "source": [
    "# Function to apply padding and create a mask\n",
    "def pad_sequences(sequences, max_len=None, padding_value=0):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "    padded_seqs = np.full((len(sequences), max_len), padding_value, dtype=np.float32)\n",
    "    masks = np.zeros((len(sequences), max_len), dtype=np.float32)\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = len(seq)\n",
    "        padded_seqs[i, :seq_len] = seq\n",
    "        masks[i, :seq_len] = 1  # Valid data points\n",
    "\n",
    "    return padded_seqs, masks\n",
    "\n",
    "# Bandpass filter for seismic data\n",
    "def apply_bandpass_filter(trace, lowcut=0.5, highcut=1.0, sampling_rate=6.625, order=4):\n",
    "    sos = signal.butter(order, [lowcut, highcut], btype='bandpass', fs=sampling_rate, output='sos')\n",
    "    filtered_trace = signal.sosfilt(sos, trace)\n",
    "    return filtered_trace\n",
    "\n",
    "# STA/LTA feature extraction\n",
    "def extract_sta_lta_features(trace, sampling_rate, sta_window=1.0, lta_window=5.0, fixed_length=500):\n",
    "    sta_samples = int(sta_window * sampling_rate)\n",
    "    lta_samples = int(lta_window * sampling_rate)\n",
    "    cft = classic_sta_lta(trace, sta_samples, lta_samples)\n",
    "    \n",
    "    if len(cft) > fixed_length:\n",
    "        features = cft[:fixed_length]  # Truncate if longer\n",
    "    else:\n",
    "        features = np.pad(cft, (0, fixed_length - len(cft)), 'constant')  # Pad with zeros if shorter\n",
    "    \n",
    "    return features\n",
    "\n",
    "# FFT feature extraction\n",
    "def extract_spectral_features(trace, sampling_rate, n_fft=256):\n",
    "    \"\"\"\n",
    "    Extract spectral features using FFT.\n",
    "    \n",
    "    Parameters:\n",
    "    - trace: Seismic data trace\n",
    "    - sampling_rate: Sampling rate of the trace\n",
    "    - n_fft: Number of FFT points\n",
    "    \n",
    "    Returns:\n",
    "    - spectral_features: Extracted spectral features\n",
    "    \"\"\"\n",
    "    # Perform FFT on the trace\n",
    "    f, t, Sxx = signal.spectrogram(trace, fs=sampling_rate, nfft=n_fft)\n",
    "    spectral_features = Sxx.mean(axis=1)  # Mean over time to reduce dimensionality\n",
    "    return spectral_features\n",
    "\n",
    "# Data augmentation (add random noise)\n",
    "def augment_data(trace):\n",
    "    noise = np.random.normal(0, 0.01, len(trace))\n",
    "    augmented_trace = trace + noise\n",
    "    return augmented_trace\n",
    "\n",
    "# Complete preprocessing function\n",
    "def preprocess_seismic_data(filepath, filetype, sampling_rate=6.625):\n",
    "    if filetype == 'csv':\n",
    "        seismic_data = pd.read_csv(filepath)\n",
    "        trace = seismic_data['velocity(m/s)'].values\n",
    "    elif filetype == 'mseed':\n",
    "        st = read(filepath)\n",
    "        trace = st[0].data\n",
    "    \n",
    "    # Apply bandpass filter\n",
    "    filtered_trace = apply_bandpass_filter(trace, sampling_rate=sampling_rate)\n",
    "    \n",
    "    # Extract STA/LTA and FFT features\n",
    "    sta_lta_features = extract_sta_lta_features(filtered_trace, sampling_rate)\n",
    "    fft_features = extract_spectral_features(filtered_trace, sampling_rate)\n",
    "    \n",
    "    # Combine both features into a single feature vector\n",
    "    combined_features = np.concatenate((sta_lta_features, fft_features))\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "# Load seismic data and optionally augment minority class data\n",
    "def load_seismic_data(data_dir, catalog_df=None, include_catalog=False, augment_minority_class=False):\n",
    "    seismic_data = []\n",
    "    labels = []\n",
    "    \n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            if file.endswith('.mseed'):\n",
    "                filetype = 'mseed'\n",
    "            elif file.endswith('.csv'):\n",
    "                filetype = 'csv'\n",
    "            else:\n",
    "                continue  # Skip unsupported file types\n",
    "            \n",
    "            # Preprocess seismic data (bandpass filtering and STA/LTA + FFT features)\n",
    "            features = preprocess_seismic_data(filepath, filetype)\n",
    "            seismic_data.append(features)\n",
    "            \n",
    "            # Add labels from catalog if included\n",
    "            if include_catalog and catalog_df is not None:\n",
    "                event_id = os.path.splitext(file)[0]\n",
    "                if event_id in catalog_df['filename'].values:\n",
    "                    label_row = catalog_df.loc[catalog_df['filename'] == event_id]\n",
    "                    labels.append(label_row['mq_type'].values[0])  # Extract the string label\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_data, masks = pad_sequences(seismic_data)\n",
    "\n",
    "    if include_catalog:\n",
    "        # Encode labels to numeric values\n",
    "        label_encoder = LabelEncoder()\n",
    "        labels_encoded = label_encoder.fit_transform(labels)  # Convert labels to integers\n",
    "        \n",
    "        # Augment minority class data if specified\n",
    "        if augment_minority_class:\n",
    "            minority_class_indices = np.where(labels_encoded == np.min(labels_encoded))[0]\n",
    "            augmented_seismic_data = [augment_data(seismic_data[idx]) for idx in minority_class_indices]\n",
    "            padded_data = np.concatenate((padded_data, augmented_seismic_data), axis=0)\n",
    "            labels_encoded = np.concatenate((labels_encoded, labels_encoded[minority_class_indices]), axis=0)\n",
    "        \n",
    "        return padded_data, labels_encoded, masks\n",
    "    else:\n",
    "        return padded_data, masks\n",
    "\n",
    "# CNN architecture with dropout and global average pooling\n",
    "class SeismicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SeismicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout for regularization\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        self.fc1 = None  # Dynamically initialized later\n",
    "        self.fc2 = None  # Dynamically initialized later\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.global_pool(x)  # Global pooling instead of flattening\n",
    "        x = x.view(x.size(0), -1)  # Flatten after pooling\n",
    "        \n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(x.size(1), 100)  # Dynamically initialize fully connected layer\n",
    "            self.fc2 = nn.Linear(100, 3)  # For 3 classes\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))  # Fully connected layer\n",
    "        x = self.fc2(x)  # Output layer (logits for 3 classes)\n",
    "        return x\n",
    "\n",
    "# Prepare data for PyTorch (ensure correct shape)\n",
    "def prepare_data_for_pytorch(X, y):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # Add channel dimension (batch_size, channels, features)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)  # Long tensor for labels\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "# Train CNN model for multi-class classification\n",
    "def train_cnn_model(train_loader, y_train, num_epochs=50, learning_rate=0.0001):  # Reduced LR and epochs\n",
    "    model = SeismicCNN().to(device)\n",
    "    \n",
    "    # Calculate class weights for imbalanced data\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Using class weights in the loss function\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {running_loss:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "\n",
    "    print(\"\\nTest Set Evaluation Metrics:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    print(f\"Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Oversampling the minority classes after preprocessing\n",
    "def oversample_data(X_train, y_train):\n",
    "    sm = SMOTE(n_neighbors=1, random_state=42)  # Set n_neighbors to 1\n",
    "    X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "    return X_res, y_res\n",
    "\n",
    "\n",
    "\n",
    "# Modify main() to use the oversample_data function\n",
    "def main():\n",
    "    catalog_path = '../../data/lunar_data/training/catalogs/apollo12_catalog_GradeA_final.csv'\n",
    "    data_directory = '../../data/lunar_data/training/data/'\n",
    "\n",
    "    # Load and preprocess data\n",
    "    print(\"Preprocessing data...\")\n",
    "    catalog = pd.read_csv(catalog_path)\n",
    "    X, y, _ = load_seismic_data(data_directory, catalog_df=catalog, include_catalog=True, augment_minority_class=False)\n",
    "\n",
    "    # Train/Test split (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Oversample minority classes in the training set\n",
    "    X_train_res, y_train_res = oversample_data(X_train, y_train)\n",
    "\n",
    "    print(f\"Shape of training data: {X_train_res.shape}, Test data: {X_test.shape}\")\n",
    "\n",
    "    # Prepare data for PyTorch\n",
    "    train_loader = prepare_data_for_pytorch(X_train_res, y_train_res)\n",
    "    test_loader = prepare_data_for_pytorch(X_test, y_test)\n",
    "\n",
    "    # Train CNN\n",
    "    print(\"Training CNN...\")\n",
    "    cnn_model = train_cnn_model(train_loader, y_train_res, num_epochs=100)\n",
    "    print(\"CNN training complete.\")\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    print(\"Evaluating CNN on test set...\")\n",
    "    evaluate_model(cnn_model, test_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
