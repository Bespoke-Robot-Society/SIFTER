{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read\n",
    "from scipy import signal\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply padding and create a mask\n",
    "def pad_sequences(sequences, max_len=None, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad each sequence to the maximum length with a specified padding value and create a mask.\n",
    "    \"\"\"\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "    padded_seqs = np.full((len(sequences), max_len), padding_value, dtype=np.float32)\n",
    "    masks = np.zeros((len(sequences), max_len), dtype=np.float32)\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = len(seq)\n",
    "        padded_seqs[i, :seq_len] = seq\n",
    "        masks[i, :seq_len] = 1  # Valid data points\n",
    "\n",
    "    return padded_seqs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandpass filter for seismic data\n",
    "def apply_bandpass_filter(trace, lowcut=0.5, highcut=3.0, sampling_rate=6.625, order=4):\n",
    "    sos = signal.butter(order, [lowcut, highcut], btype='bandpass', fs=sampling_rate, output='sos')\n",
    "    filtered_trace = signal.sosfilt(sos, trace)\n",
    "    return filtered_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STA/LTA feature extraction\n",
    "def extract_sta_lta_features(trace, sampling_rate, sta_window=1.0, lta_window=5.0, fixed_length=500):\n",
    "    sta_samples = int(sta_window * sampling_rate)\n",
    "    lta_samples = int(lta_window * sampling_rate)\n",
    "    cft = classic_sta_lta(trace, sta_samples, lta_samples)\n",
    "    \n",
    "    if len(cft) > fixed_length:\n",
    "        features = cft[:fixed_length]  # Truncate if longer\n",
    "    else:\n",
    "        features = np.pad(cft, (0, fixed_length - len(cft)), 'constant')  # Pad with zeros if shorter\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete preprocessing function\n",
    "def preprocess_seismic_data(filepath, filetype, sampling_rate=6.625):\n",
    "    if filetype == 'csv':\n",
    "        seismic_data = pd.read_csv(filepath)\n",
    "        trace = seismic_data['velocity(m/s)'].values\n",
    "    elif filetype == 'mseed':\n",
    "        st = read(filepath)\n",
    "        trace = st[0].data\n",
    "    \n",
    "    filtered_trace = apply_bandpass_filter(trace, sampling_rate=sampling_rate)\n",
    "    features = extract_sta_lta_features(filtered_trace, sampling_rate)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seismic_data(data_dir, catalog_df=None, include_catalog=False):\n",
    "    seismic_data = []\n",
    "    labels = []\n",
    "    \n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            if file.endswith('.csv'):\n",
    "                filetype = 'csv'\n",
    "            elif file.endswith('.mseed'):\n",
    "                filetype = 'mseed'\n",
    "            else:\n",
    "                continue  # Skip unsupported file types\n",
    "            \n",
    "            # Preprocess seismic data (bandpass filtering and STA/LTA)\n",
    "            features = preprocess_seismic_data(filepath, filetype)\n",
    "            seismic_data.append(features)\n",
    "            \n",
    "            if include_catalog and catalog_df is not None:\n",
    "                event_id = os.path.splitext(file)[0]\n",
    "                if event_id in catalog_df['filename'].values:\n",
    "                    label_row = catalog_df.loc[catalog_df['filename'] == event_id]\n",
    "                    labels.append(label_row['mq_type'].values[0])  # Extract the string label\n",
    "    \n",
    "    # Convert seismic data to NumPy array\n",
    "    padded_data, masks = pad_sequences(seismic_data)\n",
    "\n",
    "    if include_catalog:\n",
    "        # Encode labels to numeric values\n",
    "        label_encoder = LabelEncoder()\n",
    "        labels_encoded = label_encoder.fit_transform(labels)  # Convert labels to integers\n",
    "        return padded_data, labels_encoded, masks\n",
    "    else:\n",
    "        return padded_data, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_decision_tree(X, y, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "    dt_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = dt_model.predict(X_test)\n",
    "\n",
    "    print(\"Classification Report for Decision Tree:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return dt_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    catalog_path = '../../data/lunar_data/training/catalogs/apollo12_catalog_GradeA_final.csv'\n",
    "    data_directory = '../../data/lunar_data/training/data/'\n",
    "\n",
    "    # Load and preprocess data\n",
    "    print(\"Preprocessing data...\")\n",
    "    catalog = pd.read_csv(catalog_path)\n",
    "    X, y, _ = load_seismic_data(data_directory, catalog_df=catalog, include_catalog=True)\n",
    "\n",
    "    # Now `y` is a NumPy array of encoded labels, so we can check its shape\n",
    "    print(f\"Shape of features (X): {X.shape}\")\n",
    "    print(f\"Shape of labels (y): {y.shape}\")\n",
    "    print(f\"Unique labels: {np.unique(y)}\")  # Optional: Check the unique label values\n",
    "\n",
    "    if X.shape[0] == 0 or y.shape[0] == 0:\n",
    "        print(\"No data to train on!\")\n",
    "        return\n",
    "\n",
    "    # Train and evaluate Decision Tree\n",
    "    print(\"Training Decision Tree...\")\n",
    "    dt_model = train_and_evaluate_decision_tree(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Shape of features (X): (152, 500)\n",
      "Shape of labels (y): (152,)\n",
      "Unique labels: [0 1 2]\n",
      "Training Decision Tree...\n",
      "Classification Report for Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.67      0.57         3\n",
      "           1       0.96      0.93      0.94        27\n",
      "           2       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.90        31\n",
      "   macro avg       0.82      0.86      0.84        31\n",
      "weighted avg       0.92      0.90      0.91        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
