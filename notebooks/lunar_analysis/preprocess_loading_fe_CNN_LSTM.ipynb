{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "from scipy import signal\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert relative time to absolute time\n",
    "def convert_rel_to_abs_time(start_time, time_rel):\n",
    "    \"\"\"\n",
    "    Convert relative time to absolute time using the trace start time.\n",
    "    \"\"\"\n",
    "    return (start_time + timedelta(seconds=float(time_rel))).strftime('%Y-%m-%dT%H:%M:%S.%f')\n",
    "\n",
    "# Function to apply a bandpass filter to a seismic trace\n",
    "def apply_bandpass_filter(trace, lowcut=0.01, highcut=3.0, sampling_rate=6.625, order=4):\n",
    "    \"\"\"\n",
    "    Apply a bandpass filter to seismic trace to remove noise outside the frequency range.\n",
    "    \"\"\"\n",
    "    sos = signal.butter(order, [lowcut, highcut], btype='bandpass', fs=sampling_rate, output='sos')\n",
    "    filtered_trace = signal.sosfilt(sos, trace)\n",
    "    return filtered_trace\n",
    "\n",
    "# STA/LTA Feature Extraction\n",
    "def extract_sta_lta_features(trace, sampling_rate, sta_window=1.0, lta_window=5.0):\n",
    "    \"\"\"\n",
    "    Extract STA/LTA features to detect seismic events.\n",
    "    \"\"\"\n",
    "    sta_samples = int(sta_window * sampling_rate)\n",
    "    lta_samples = int(lta_window * sampling_rate)\n",
    "    cft = classic_sta_lta(trace, sta_samples, lta_samples)\n",
    "    \n",
    "    return cft  # STA/LTA characteristic function\n",
    "\n",
    "# Normalization/Standardization\n",
    "def normalize_trace(trace, method='standard'):\n",
    "    \"\"\"\n",
    "    Normalize or standardize seismic data.\n",
    "    \n",
    "    - 'standard': standardizes the data (mean = 0, std = 1)\n",
    "    - 'minmax': normalizes the data to range [0, 1]\n",
    "    \"\"\"\n",
    "    if method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid normalization method. Choose 'standard' or 'minmax'.\")\n",
    "    \n",
    "    trace = trace.reshape(-1, 1)  # Reshape trace for scaler\n",
    "    trace_normalized = scaler.fit_transform(trace).flatten()  # Normalize and flatten back\n",
    "    return trace_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to segment seismic events\n",
    "def segment_seismic_events(trace, segment_length=3600, sampling_rate=6.625):\n",
    "    \"\"\"\n",
    "    Segment seismic trace into smaller parts based on segment length (in seconds).\n",
    "    \n",
    "    Parameters:\n",
    "    - trace: Seismic data (1-hour or 1-day trace).\n",
    "    - segment_length: Length of each segment in seconds.\n",
    "    - sampling_rate: Sampling rate of the data (samples per second).\n",
    "    \n",
    "    Returns:\n",
    "    - List of segmented traces.\n",
    "    \"\"\"\n",
    "    samples_per_segment = int(segment_length * sampling_rate)\n",
    "    num_segments = len(trace) // samples_per_segment\n",
    "    \n",
    "    segmented_traces = []\n",
    "    for i in range(num_segments):\n",
    "        start = i * samples_per_segment\n",
    "        end = (i + 1) * samples_per_segment\n",
    "        segmented_traces.append(trace[start:end])\n",
    "    \n",
    "    return segmented_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_seismic_data(filepath, filetype, sampling_rate=6.625, normalization_method='standard'):\n",
    "    \"\"\"\n",
    "    Preprocess seismic data from .csv or .mseed files:\n",
    "    - Apply bandpass filter\n",
    "    - Extract both time-domain and frequency-domain features\n",
    "    - Normalize the filtered trace\n",
    "    \"\"\"\n",
    "    # Read the trace from file\n",
    "    if filetype == 'csv':\n",
    "        seismic_data = pd.read_csv(filepath)\n",
    "        trace = seismic_data['velocity(c/s)'].values\n",
    "        time_rel = seismic_data['rel_time(sec)'].values\n",
    "    elif filetype == 'mseed':\n",
    "        st = read(filepath)\n",
    "        tr = st[0]\n",
    "        trace = tr.data\n",
    "        time_rel = tr.times()\n",
    "\n",
    "    # Apply bandpass filter\n",
    "    filtered_trace = apply_bandpass_filter(trace, sampling_rate=sampling_rate)\n",
    "\n",
    "    # Normalize the filtered trace\n",
    "    normalized_trace = normalize_trace(filtered_trace, method=normalization_method)\n",
    "\n",
    "    # Extract time-domain features\n",
    "    sta_lta_features, envelope, velocity, acceleration = extract_time_domain_features(normalized_trace, sampling_rate)\n",
    "\n",
    "    # Extract frequency-domain features\n",
    "    frequencies, times, spectrogram_data, fft_values, frequencies_psd, psd = extract_frequency_domain_features(normalized_trace, sampling_rate)\n",
    "\n",
    "    # Convert relative times to absolute times (for labeled data)\n",
    "    if filetype == 'mseed':\n",
    "        starttime = tr.stats.starttime.datetime\n",
    "        time_abs = [convert_rel_to_abs_time(starttime, rel_time) for rel_time in time_rel]\n",
    "    else:\n",
    "        time_abs = time_rel  # For CSV, time_rel is already present.\n",
    "    \n",
    "    return (normalized_trace, sta_lta_features, envelope, velocity, acceleration, \n",
    "            frequencies, times, spectrogram_data, fft_values, frequencies_psd, psd, time_abs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import hilbert\n",
    "\n",
    "# Function to compute the envelope of a seismic signal\n",
    "def compute_envelope(trace):\n",
    "    \"\"\"\n",
    "    Compute the envelope of the signal using Hilbert transform.\n",
    "    \"\"\"\n",
    "    analytic_signal = hilbert(trace)\n",
    "    envelope = np.abs(analytic_signal)\n",
    "    return envelope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the first and second derivative (velocity and acceleration)\n",
    "def compute_derivatives(trace, sampling_rate):\n",
    "    \"\"\"\n",
    "    Compute velocity (1st derivative) and acceleration (2nd derivative) of the seismic signal.\n",
    "    \"\"\"\n",
    "    dt = 1 / sampling_rate\n",
    "    velocity = np.gradient(trace, dt)  # 1st derivative\n",
    "    acceleration = np.gradient(velocity, dt)  # 2nd derivative\n",
    "    return velocity, acceleration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import spectrogram\n",
    "\n",
    "def compute_spectrogram(trace, sampling_rate, nperseg=256, noverlap=128):\n",
    "    \"\"\"\n",
    "    Compute the spectrogram of the seismic signal to extract time-frequency features.\n",
    "    \"\"\"\n",
    "    frequencies, times, Sxx = spectrogram(trace, fs=sampling_rate, nperseg=nperseg, noverlap=noverlap)\n",
    "    return frequencies, times, Sxx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "\n",
    "def compute_fft(trace):\n",
    "    \"\"\"\n",
    "    Compute the Fast Fourier Transform (FFT) of the seismic signal.\n",
    "    \"\"\"\n",
    "    fft_values = np.abs(fft(trace))\n",
    "    return fft_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "\n",
    "def compute_psd(trace, sampling_rate):\n",
    "    \"\"\"\n",
    "    Compute the Power Spectral Density (PSD) using Welch's method.\n",
    "    \"\"\"\n",
    "    frequencies, psd = welch(trace, fs=sampling_rate, nperseg=256)\n",
    "    return frequencies, psd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_domain_features(trace, sampling_rate):\n",
    "    \"\"\"\n",
    "    Extract time-domain features from the seismic trace.\n",
    "    Features:\n",
    "    - STA/LTA ratio\n",
    "    - Envelope (amplitude)\n",
    "    - Velocity and Acceleration (derivatives)\n",
    "    \"\"\"\n",
    "    # STA/LTA feature (already done)\n",
    "    sta_lta_features = extract_sta_lta_features(trace, sampling_rate)\n",
    "\n",
    "    # Envelope (signal amplitude)\n",
    "    envelope = compute_envelope(trace)\n",
    "\n",
    "    # Velocity and Acceleration (derivatives)\n",
    "    velocity, acceleration = compute_derivatives(trace, sampling_rate)\n",
    "\n",
    "    return sta_lta_features, envelope, velocity, acceleration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frequency_domain_features(trace, sampling_rate):\n",
    "    \"\"\"\n",
    "    Extract frequency-domain features from the seismic trace.\n",
    "    Features:\n",
    "    - Spectrogram\n",
    "    - Fourier Transform (FFT)\n",
    "    - Power Spectral Density (PSD)\n",
    "    \"\"\"\n",
    "    # Spectrogram (time-frequency analysis)\n",
    "    frequencies, times, spectrogram_data = compute_spectrogram(trace, sampling_rate)\n",
    "\n",
    "    # FFT (frequency-domain analysis)\n",
    "    fft_values = compute_fft(trace)\n",
    "\n",
    "    # Power Spectral Density (PSD)\n",
    "    frequencies_psd, psd = compute_psd(trace, sampling_rate)\n",
    "\n",
    "    return frequencies, times, spectrogram_data, fft_values, frequencies_psd, psd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lunar_data(catalog_path, data_dir):\n",
    "    \"\"\"\n",
    "    Load lunar seismic data from the catalog and associated .mseed files.\n",
    "    Only .mseed files are loaded, and the catalog is used for labeling.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_path: Path to the lunar catalog CSV file.\n",
    "    - data_dir: Directory containing the lunar .mseed files.\n",
    "    \n",
    "    Returns:\n",
    "    - lunar_data: List of loaded seismic traces.\n",
    "    - lunar_labels: List of labels (from the catalog).\n",
    "    - lunar_arrival_times: List of arrival times (absolute).\n",
    "    \"\"\"\n",
    "    catalog = pd.read_csv(catalog_path)\n",
    "    lunar_data = []\n",
    "    lunar_labels = []\n",
    "    lunar_arrival_times = []\n",
    "    \n",
    "    for _, row in catalog.iterrows():\n",
    "        filename = row['filename'] + '.mseed'\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Loading MSEED file: {file_path}\")\n",
    "            st = read(file_path)\n",
    "            trace = st[0].data\n",
    "            lunar_data.append(trace)  # Extract the trace data\n",
    "            lunar_labels.append(row['mq_type'])  # Label from the catalog\n",
    "            lunar_arrival_times.append(row['time_abs(%Y-%m-%dT%H:%M:%S.%f)'])  # Arrival time from catalog\n",
    "        else:\n",
    "            print(f\"File {filename} not found.\")\n",
    "    \n",
    "    return lunar_data, lunar_labels, lunar_arrival_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_martian_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load martian seismic data from the .mseed files (unlabeled).\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dir: Directory containing the martian .mseed files.\n",
    "    \n",
    "    Returns:\n",
    "    - martian_data: List of processed seismic traces and features.\n",
    "    - martian_time_abs: List of absolute times corresponding to seismic events.\n",
    "    \"\"\"\n",
    "    martian_data = []\n",
    "    martian_time_abs = []\n",
    "\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.mseed'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Loading MSEED file: {file_path}\")\n",
    "\n",
    "                # Adjusting to capture all returned values from preprocess_seismic_data\n",
    "                (\n",
    "                    filtered_trace, sta_lta_features, envelope, velocity, acceleration, \n",
    "                    frequencies, times, spectrogram_data, fft_values, frequencies_psd, psd, time_abs\n",
    "                ) = preprocess_seismic_data(file_path, filetype='mseed')\n",
    "                \n",
    "                # Store all relevant features for each trace\n",
    "                martian_data.append({\n",
    "                    'trace': filtered_trace,\n",
    "                    'sta_lta': sta_lta_features,\n",
    "                    'envelope': envelope,\n",
    "                    'velocity': velocity,\n",
    "                    'acceleration': acceleration,\n",
    "                    'spectrogram': spectrogram_data,\n",
    "                    'fft': fft_values,\n",
    "                    'psd': psd\n",
    "                })\n",
    "                \n",
    "                martian_time_abs.append(time_abs)  # Append absolute times for each trace\n",
    "    \n",
    "    return martian_data, martian_time_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def convert_time_to_seconds(time_list, reference_time=None):\n",
    "    \"\"\"\n",
    "    Convert a list of absolute times (e.g., '1970-01-01T12:34:56.789') to seconds since a reference time.\n",
    "    If no reference time is provided, the earliest time in the list is used as the reference.\n",
    "    \n",
    "    Parameters:\n",
    "    - time_list: List of absolute time strings.\n",
    "    - reference_time: Optional. A reference time (datetime object) to calculate time differences.\n",
    "    \n",
    "    Returns:\n",
    "    - List of time differences in seconds since the reference time.\n",
    "    \"\"\"\n",
    "    # Flatten the time_list (in case it's a list of lists)\n",
    "    flat_time_list = [item for sublist in time_list for item in sublist] if isinstance(time_list[0], list) else time_list\n",
    "\n",
    "    # Convert the list to datetime objects\n",
    "    time_list_dt = [datetime.strptime(t, '%Y-%m-%dT%H:%M:%S.%f') for t in flat_time_list]\n",
    "    \n",
    "    if reference_time is None:\n",
    "        # Set reference time to the earliest time in the list\n",
    "        reference_time = min(time_list_dt)\n",
    "\n",
    "    # Convert each time to seconds since the reference time\n",
    "    time_in_seconds = [(t - reference_time).total_seconds() for t in time_list_dt]\n",
    "    \n",
    "    return time_in_seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate_sequence(sequence, target_length):\n",
    "    \"\"\"\n",
    "    Pad the sequence with zeros or truncate it to match the target length.\n",
    "    \"\"\"\n",
    "    if len(sequence) > target_length:\n",
    "        return sequence[:target_length]  # Truncate if longer than target length\n",
    "    elif len(sequence) < target_length:\n",
    "        # Pad with zeros if shorter\n",
    "        padding = np.zeros(target_length - len(sequence))\n",
    "        return np.concatenate((sequence, padding))\n",
    "    else:\n",
    "        return sequence  # Return as is if already the correct length\n",
    "\n",
    "def prepare_data_for_multitask(X, y_event, y_time, target_length=None):\n",
    "    \"\"\"\n",
    "    Prepare data for PyTorch by padding/truncating all sequences to the same length.\n",
    "    \n",
    "    - X: List of input sequences (seismic traces).\n",
    "    - y_event: List of event type labels.\n",
    "    - y_time: List of time labels (in seconds).\n",
    "    - target_length: The target length for all sequences. If None, use the length of the longest sequence.\n",
    "    \"\"\"\n",
    "    # Find the target length for padding (either given or based on the longest sequence)\n",
    "    if target_length is None:\n",
    "        target_length = max([len(x) for x in X])\n",
    "\n",
    "    # Pad or truncate all sequences to match the target length\n",
    "    X_padded = [pad_or_truncate_sequence(x, target_length) for x in X]\n",
    "    \n",
    "    # Convert the lists to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X_padded, dtype=torch.float32).unsqueeze(1)  # Add channel dimension for CNN input\n",
    "    y_event_tensor = torch.tensor(y_event, dtype=torch.long)  # Event type labels\n",
    "    y_time_tensor = torch.tensor(y_time, dtype=torch.float32)  # Arrival time labels (now in numerical format)\n",
    "    \n",
    "    # Combine into a DataLoader for PyTorch\n",
    "    dataset = torch.utils.data.TensorDataset(X_tensor, y_event_tensor, y_time_tensor)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_multitask(model, train_loader, val_loader, criterion_event, criterion_time, optimizer, num_epochs=100, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Train a multitask model on the given data loaders.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The neural network model to train.\n",
    "    - train_loader: DataLoader for training data.\n",
    "    - val_loader: DataLoader for validation data.\n",
    "    - criterion_event: Loss function for event classification (cross-entropy loss).\n",
    "    - criterion_time: Loss function for time prediction (MSE or MAE).\n",
    "    - optimizer: Optimizer for training the model (e.g., Adam).\n",
    "    - num_epochs: Number of epochs to train.\n",
    "    - device: Device to use for training ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - trained_model: The trained model after the specified number of epochs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, event_labels, time_labels in train_loader:\n",
    "            inputs, event_labels, time_labels = inputs.to(device), event_labels.to(device), time_labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            event_output, time_output = model(inputs)\n",
    "\n",
    "            # Compute the loss for both tasks\n",
    "            loss_event = criterion_event(event_output, event_labels)\n",
    "            loss_time = criterion_time(time_output.squeeze(), time_labels)  # time_output might need to be squeezed if it has an extra dimension\n",
    "            loss = loss_event + loss_time\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print loss for this epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "        # Optionally, add validation loop here to monitor performance on val_loader\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMHybridModel(nn.Module):\n",
    "    def __init__(self, input_size=1, num_classes=3, hidden_size=128, num_lstm_layers=2, dropout_prob=0.3):\n",
    "        super(CNNLSTMHybridModel, self).__init__()\n",
    "\n",
    "        # CNN layers to capture spatial patterns from seismic data\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # LSTM layers to capture temporal dependencies\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=64,  # Input to LSTM is the output from CNN\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layers for event classification and arrival time regression (multi-task learning)\n",
    "        self.fc_event = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 128),  # *2 for bidirectional\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(128, num_classes)  # Output: number of classes (3: impact, shallow, deep)\n",
    "        )\n",
    "\n",
    "        self.fc_time = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 128),  # *2 for bidirectional\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(128, 1)  # Output: arrival time prediction (regression)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (batch_size, channels, seq_length) for CNN\n",
    "        cnn_out = self.cnn(x)  # Shape: (batch_size, 64, reduced_seq_length)\n",
    "\n",
    "        # Prepare input for LSTM: (batch_size, seq_length, features)\n",
    "        cnn_out = cnn_out.permute(0, 2, 1)  # Shape: (batch_size, reduced_seq_length, 64)\n",
    "\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(cnn_out)  # Shape: (batch_size, reduced_seq_length, hidden_size * 2)\n",
    "\n",
    "        # Take the output from the last time step\n",
    "        lstm_out = lstm_out[:, -1, :]  # Shape: (batch_size, hidden_size * 2)\n",
    "\n",
    "        # Event classification head\n",
    "        event_output = self.fc_event(lstm_out)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        # Arrival time regression head\n",
    "        time_output = self.fc_time(lstm_out)  # Shape: (batch_size, 1)\n",
    "\n",
    "        return event_output, time_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define paths to lunar data\n",
    "    lunar_catalog_path = '../../data/lunar_data/training/catalogs/apollo12_catalog_GradeA_final.csv'\n",
    "    lunar_data_directory = '../../data/lunar_data/training/data/S12_GradeA/'\n",
    "\n",
    "    # Load lunar data\n",
    "    print(\"Loading and preprocessing Lunar MSEED data...\")\n",
    "    lunar_data, lunar_labels, lunar_time_abs = load_lunar_data(lunar_catalog_path, lunar_data_directory)\n",
    "    print(f\"Lunar Data Loaded: {len(lunar_data)} traces.\")\n",
    "    \n",
    "    # Convert the absolute times (lunar_time_abs) into seconds since the start of the event\n",
    "    y_time_train_seconds = convert_time_to_seconds(lunar_time_abs)\n",
    "\n",
    "    # Ensure that the sizes match for lunar_data, lunar_labels, and y_time_train_seconds\n",
    "    print(f\"Data size check: {len(lunar_data)} data, {len(lunar_labels)} labels, {len(y_time_train_seconds)} times.\")\n",
    "\n",
    "    # Convert string labels in lunar_labels to integer labels using LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    lunar_labels_encoded = label_encoder.fit_transform(lunar_labels)\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_event_train, y_event_val, y_time_train, y_time_val = train_test_split(\n",
    "        lunar_data, lunar_labels_encoded, y_time_train_seconds, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Prepare data for PyTorch (multitask model: event type prediction + arrival time prediction)\n",
    "    train_loader = prepare_data_for_multitask(X_train, y_event_train, y_time_train)\n",
    "    val_loader = prepare_data_for_multitask(X_val, y_event_val, y_time_val)\n",
    "\n",
    "    # Initialize the CNN-LSTM Hybrid model for multitask learning\n",
    "    model = CNNLSTMHybridModel().to(device)\n",
    "    \n",
    "    # Define optimizer and loss functions (cross-entropy for event classification, MAE for time prediction)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion_event = nn.CrossEntropyLoss()\n",
    "    criterion_time = nn.MSELoss()\n",
    "\n",
    "    # Train the model using the training and validation data\n",
    "    print(\"Training model on Lunar dataset...\")\n",
    "    trained_model = train_model_multitask(model, train_loader, val_loader, criterion_event, criterion_time, optimizer, num_epochs=100, device=device)\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(trained_model.state_dict(), 'pretrained_lunar_model.pth')\n",
    "    print(\"Model training complete and saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
