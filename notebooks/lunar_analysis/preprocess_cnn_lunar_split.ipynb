{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read\n",
    "from scipy import signal\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Set device to CPU\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply padding and create a mask\n",
    "def pad_sequences(sequences, max_len=None, padding_value=0):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "    padded_seqs = np.full((len(sequences), max_len), padding_value, dtype=np.float32)\n",
    "    masks = np.zeros((len(sequences), max_len), dtype=np.float32)\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = len(seq)\n",
    "        padded_seqs[i, :seq_len] = seq\n",
    "        masks[i, :seq_len] = 1  # Valid data points\n",
    "\n",
    "    return padded_seqs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandpass filter for seismic data\n",
    "def apply_bandpass_filter(trace, lowcut=0.5, highcut=1.0, sampling_rate=6.625, order=4):\n",
    "    sos = signal.butter(order, [lowcut, highcut], btype='bandpass', fs=sampling_rate, output='sos')\n",
    "    filtered_trace = signal.sosfilt(sos, trace)\n",
    "    return filtered_trace\n",
    "\n",
    "# STA/LTA feature extraction\n",
    "def extract_sta_lta_features(trace, sampling_rate, sta_window=1.0, lta_window=5.0, fixed_length=500):\n",
    "    sta_samples = int(sta_window * sampling_rate)\n",
    "    lta_samples = int(lta_window * sampling_rate)\n",
    "    cft = classic_sta_lta(trace, sta_samples, lta_samples)\n",
    "    \n",
    "    if len(cft) > fixed_length:\n",
    "        features = cft[:fixed_length]  # Truncate if longer\n",
    "    else:\n",
    "        features = np.pad(cft, (0, fixed_length - len(cft)), 'constant')  # Pad with zeros if shorter\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess seismic data and extract STA/LTA features\n",
    "def preprocess_seismic_data(filepath, filetype, sampling_rate=6.625):\n",
    "    if filetype == 'csv':\n",
    "        seismic_data = pd.read_csv(filepath)\n",
    "        trace = seismic_data['velocity(m/s)'].values\n",
    "    elif filetype == 'mseed':\n",
    "        st = read(filepath)\n",
    "        trace = st[0].data\n",
    "    \n",
    "    # Apply bandpass filter to trace\n",
    "    filtered_trace = apply_bandpass_filter(trace, sampling_rate=sampling_rate)\n",
    "    \n",
    "    # Extract STA/LTA features and find arrival time\n",
    "    cft, arrival_time_rel = extract_sta_lta_features(filtered_trace, sampling_rate)\n",
    "    \n",
    "    return filtered_trace, cft, arrival_time_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load seismic data and catalog for arrival time processing\n",
    "def load_seismic_data(data_dir, catalog_df=None, include_catalog=False, max_len=500):\n",
    "    seismic_data = []\n",
    "    labels = []\n",
    "    \n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            if file.endswith('.mseed'):\n",
    "                filetype = 'mseed'\n",
    "            elif file.endswith('.csv'):\n",
    "                filetype = 'csv'\n",
    "            else:\n",
    "                continue  # Skip unsupported file types\n",
    "            \n",
    "            # Preprocess seismic data and extract STA/LTA\n",
    "            filtered_trace, cft, arrival_time_rel = preprocess_seismic_data(filepath, filetype)\n",
    "            seismic_data.append(filtered_trace)  # Save filtered traces\n",
    "            \n",
    "            # Use catalog to append labels (arrival times)\n",
    "            if include_catalog and catalog_df is not None:\n",
    "                event_id = os.path.splitext(file)[0]\n",
    "                if event_id in catalog_df['filename'].values:\n",
    "                    label_row = catalog_df.loc[catalog_df['filename'] == event_id]\n",
    "                    labels.append(label_row['time_abs(%Y-%m-%dT%H:%M:%S.%f)'].values[0])  # Extract absolute arrival time\n",
    "    \n",
    "    # Pad sequences to ensure all traces are of the same length\n",
    "    padded_data, _ = pad_sequences(seismic_data, max_len=max_len)\n",
    "\n",
    "    if include_catalog:\n",
    "        return padded_data, np.array(labels)\n",
    "    else:\n",
    "        return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeismicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SeismicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # After conv1 + pool\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # After conv2 + pool\n",
    "\n",
    "        # Flatten the output of the conv layers to pass to fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output dynamically based on batch size\n",
    "        \n",
    "        if self.fc1 is None:\n",
    "            # Dynamically initialize the fully connected layers based on the input size\n",
    "            self.fc1 = nn.Linear(x.size(1), 100)  # Use the computed flattened size\n",
    "            self.fc2 = nn.Linear(100, 3)  # For 3 classes\n",
    "\n",
    "        x = torch.relu(self.fc1(x))  # Pass through fully connected layer\n",
    "        x = self.fc2(x)  # Output layer (logits for 3 classes)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch (ensure correct shape)\n",
    "def prepare_data_for_pytorch(X, y):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # Add channel dimension (batch_size, channels, features)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)  # Long tensor for labels\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN model for multi-class classification\n",
    "def train_cnn_model(train_loader, val_loader, num_epochs=100):\n",
    "    model = SeismicCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)  # Logits output\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] complete, Total Loss: {running_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)  # Forward pass\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the predicted class\n",
    "            \n",
    "            y_true.extend(y_batch.cpu().numpy())  # Store true labels\n",
    "            y_pred.extend(predicted.cpu().numpy())  # Store predictions\n",
    "\n",
    "    # Calculate and display evaluation metrics\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1', 'Class 2']))\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    catalog_path = '../../data/lunar_data/training/catalogs/apollo12_catalog_GradeA_final.csv'\n",
    "    data_directory = '../../data/lunar_data/training/data/'\n",
    "\n",
    "    # Load and preprocess data\n",
    "    print(\"Preprocessing data...\")\n",
    "    catalog = pd.read_csv(catalog_path)\n",
    "    X, y, _ = load_seismic_data(data_directory, catalog_df=catalog, include_catalog=True)\n",
    "\n",
    "    # Train/Validation/Test split (60% train, 20% validation, 20% test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "    print(f\"Shape of training data: {X_train.shape}, Validation data: {X_val.shape}, Test data: {X_test.shape}\")\n",
    "\n",
    "    # Prepare data for PyTorch\n",
    "    train_loader = prepare_data_for_pytorch(X_train, y_train)\n",
    "    val_loader = prepare_data_for_pytorch(X_val, y_val)\n",
    "    test_loader = prepare_data_for_pytorch(X_test, y_test)\n",
    "\n",
    "    # Train CNN\n",
    "    print(\"Training CNN...\")\n",
    "    cnn_model = train_cnn_model(train_loader, val_loader, num_epochs=100)\n",
    "    print(\"CNN training complete.\")\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    print(\"Evaluating CNN on test set...\")\n",
    "    evaluate_model(cnn_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m catalog \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(catalog_path)\n\u001b[0;32m----> 8\u001b[0m X, y, _ \u001b[38;5;241m=\u001b[39m \u001b[43mload_seismic_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatalog_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatalog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_catalog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train/Validation/Test split (60% train, 20% validation, 20% test)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m X_train, X_temp, y_train, y_temp \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my)\n",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m, in \u001b[0;36mload_seismic_data\u001b[0;34m(data_dir, catalog_df, include_catalog, max_len)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip unsupported file types\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Preprocess seismic data and extract STA/LTA\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m filtered_trace, cft, arrival_time_rel \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_seismic_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiletype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m seismic_data\u001b[38;5;241m.\u001b[39mappend(filtered_trace)  \u001b[38;5;66;03m# Save filtered traces\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Use catalog to append labels (arrival times)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mpreprocess_seismic_data\u001b[0;34m(filepath, filetype, sampling_rate)\u001b[0m\n\u001b[1;32m     11\u001b[0m filtered_trace \u001b[38;5;241m=\u001b[39m apply_bandpass_filter(trace, sampling_rate\u001b[38;5;241m=\u001b[39msampling_rate)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Extract STA/LTA features and find arrival time\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m cft, arrival_time_rel \u001b[38;5;241m=\u001b[39m extract_sta_lta_features(filtered_trace, sampling_rate)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filtered_trace, cft, arrival_time_rel\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data from the training loop\n",
    "epochs = list(range(1, 101))\n",
    "train_losses = [\n",
    "    3.3378, 2.9283, 2.6316, 2.3719, 2.1595, 1.9974, 1.8743, 1.7731, 1.7275, 1.6773,\n",
    "    1.6550, 1.6350, 1.6398, 1.6162, 1.6127, 1.5282, 1.5398, 1.5448, 1.5192, 1.5318,\n",
    "    1.5179, 1.5241, 1.4718, 1.4838, 1.4495, 1.4187, 1.4091, 1.4017, 1.4140, 1.3989,\n",
    "    1.3846, 1.3787, 1.3296, 1.3468, 1.3302, 1.3196, 1.3033, 1.3280, 1.3051, 1.2995,\n",
    "    1.3079, 1.3041, 1.2648, 1.2614, 1.2522, 1.2324, 1.2393, 1.2522, 1.2206, 1.2200,\n",
    "    1.2026, 1.2139, 1.2249, 1.2037, 1.2351, 1.1769, 1.1634, 1.1634, 1.1636, 1.1250,\n",
    "    1.1274, 1.0957, 1.1422, 1.1089, 1.0969, 1.0890, 1.1023, 1.0637, 1.0400, 1.0507,\n",
    "    1.0331, 1.0087, 0.9976, 0.9925, 1.0022, 1.0183, 0.9913, 0.9706, 0.9803, 0.9458,\n",
    "    0.9403, 0.9291, 0.9663, 0.9627, 0.9335, 0.9536, 0.8982, 0.9032, 0.8985, 0.8756,\n",
    "    0.8699, 0.8616, 0.8789, 0.8596, 0.8595, 0.8409, 0.8131, 0.8206, 0.8261, 0.8191\n",
    "]\n",
    "\n",
    "# Plotting the training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, label=\"Training Loss\", color='b')\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Shape of training data: (91, 500), Validation data: (30, 500), Test data: (31, 500)\n",
      "Training CNN...\n",
      "Epoch [1/100] complete, Total Loss: 3.2546\n",
      "Epoch [2/100] complete, Total Loss: 2.9609\n",
      "Epoch [3/100] complete, Total Loss: 2.7682\n",
      "Epoch [4/100] complete, Total Loss: 2.5515\n",
      "Epoch [5/100] complete, Total Loss: 2.3803\n",
      "Epoch [6/100] complete, Total Loss: 2.2616\n",
      "Epoch [7/100] complete, Total Loss: 2.0957\n",
      "Epoch [8/100] complete, Total Loss: 1.9957\n",
      "Epoch [9/100] complete, Total Loss: 1.9162\n",
      "Epoch [10/100] complete, Total Loss: 1.8761\n",
      "Epoch [11/100] complete, Total Loss: 1.7633\n",
      "Epoch [12/100] complete, Total Loss: 1.6846\n",
      "Epoch [13/100] complete, Total Loss: 1.6444\n",
      "Epoch [14/100] complete, Total Loss: 1.6304\n",
      "Epoch [15/100] complete, Total Loss: 1.5586\n",
      "Epoch [16/100] complete, Total Loss: 1.5554\n",
      "Epoch [17/100] complete, Total Loss: 1.5017\n",
      "Epoch [18/100] complete, Total Loss: 1.4375\n",
      "Epoch [19/100] complete, Total Loss: 1.4281\n",
      "Epoch [20/100] complete, Total Loss: 1.4194\n",
      "Epoch [21/100] complete, Total Loss: 1.4100\n",
      "Epoch [22/100] complete, Total Loss: 1.4152\n",
      "Epoch [23/100] complete, Total Loss: 1.3679\n",
      "Epoch [24/100] complete, Total Loss: 1.3780\n",
      "Epoch [25/100] complete, Total Loss: 1.3692\n",
      "Epoch [26/100] complete, Total Loss: 1.3433\n",
      "Epoch [27/100] complete, Total Loss: 1.3360\n",
      "Epoch [28/100] complete, Total Loss: 1.3089\n",
      "Epoch [29/100] complete, Total Loss: 1.3029\n",
      "Epoch [30/100] complete, Total Loss: 1.2424\n",
      "Epoch [31/100] complete, Total Loss: 1.2635\n",
      "Epoch [32/100] complete, Total Loss: 1.2548\n",
      "Epoch [33/100] complete, Total Loss: 1.2259\n",
      "Epoch [34/100] complete, Total Loss: 1.1896\n",
      "Epoch [35/100] complete, Total Loss: 1.2309\n",
      "Epoch [36/100] complete, Total Loss: 1.1885\n",
      "Epoch [37/100] complete, Total Loss: 1.1882\n",
      "Epoch [38/100] complete, Total Loss: 1.1770\n",
      "Epoch [39/100] complete, Total Loss: 1.1462\n",
      "Epoch [40/100] complete, Total Loss: 1.1406\n",
      "Epoch [41/100] complete, Total Loss: 1.1413\n",
      "Epoch [42/100] complete, Total Loss: 1.1130\n",
      "Epoch [43/100] complete, Total Loss: 1.0972\n",
      "Epoch [44/100] complete, Total Loss: 1.0932\n",
      "Epoch [45/100] complete, Total Loss: 1.0766\n",
      "Epoch [46/100] complete, Total Loss: 1.0700\n",
      "Epoch [47/100] complete, Total Loss: 1.0441\n",
      "Epoch [48/100] complete, Total Loss: 1.0356\n",
      "Epoch [49/100] complete, Total Loss: 1.0307\n",
      "Epoch [50/100] complete, Total Loss: 1.0419\n",
      "Epoch [51/100] complete, Total Loss: 1.0007\n",
      "Epoch [52/100] complete, Total Loss: 1.0186\n",
      "Epoch [53/100] complete, Total Loss: 0.9836\n",
      "Epoch [54/100] complete, Total Loss: 0.9816\n",
      "Epoch [55/100] complete, Total Loss: 0.9637\n",
      "Epoch [56/100] complete, Total Loss: 0.9721\n",
      "Epoch [57/100] complete, Total Loss: 0.9315\n",
      "Epoch [58/100] complete, Total Loss: 0.9386\n",
      "Epoch [59/100] complete, Total Loss: 0.9425\n",
      "Epoch [60/100] complete, Total Loss: 0.8951\n",
      "Epoch [61/100] complete, Total Loss: 0.9010\n",
      "Epoch [62/100] complete, Total Loss: 0.8909\n",
      "Epoch [63/100] complete, Total Loss: 0.8814\n",
      "Epoch [64/100] complete, Total Loss: 0.9034\n",
      "Epoch [65/100] complete, Total Loss: 0.8768\n",
      "Epoch [66/100] complete, Total Loss: 0.9073\n",
      "Epoch [67/100] complete, Total Loss: 0.8491\n",
      "Epoch [68/100] complete, Total Loss: 0.8429\n",
      "Epoch [69/100] complete, Total Loss: 0.8284\n",
      "Epoch [70/100] complete, Total Loss: 0.8337\n",
      "Epoch [71/100] complete, Total Loss: 0.8001\n",
      "Epoch [72/100] complete, Total Loss: 0.8177\n",
      "Epoch [73/100] complete, Total Loss: 0.8058\n",
      "Epoch [74/100] complete, Total Loss: 0.8169\n",
      "Epoch [75/100] complete, Total Loss: 0.7974\n",
      "Epoch [76/100] complete, Total Loss: 0.7964\n",
      "Epoch [77/100] complete, Total Loss: 0.7746\n",
      "Epoch [78/100] complete, Total Loss: 0.7820\n",
      "Epoch [79/100] complete, Total Loss: 0.7901\n",
      "Epoch [80/100] complete, Total Loss: 0.7501\n",
      "Epoch [81/100] complete, Total Loss: 0.7451\n",
      "Epoch [82/100] complete, Total Loss: 0.7357\n",
      "Epoch [83/100] complete, Total Loss: 0.7225\n",
      "Epoch [84/100] complete, Total Loss: 0.7363\n",
      "Epoch [85/100] complete, Total Loss: 0.7212\n",
      "Epoch [86/100] complete, Total Loss: 0.7156\n",
      "Epoch [87/100] complete, Total Loss: 0.7353\n",
      "Epoch [88/100] complete, Total Loss: 0.7012\n",
      "Epoch [89/100] complete, Total Loss: 0.7026\n",
      "Epoch [90/100] complete, Total Loss: 0.6966\n",
      "Epoch [91/100] complete, Total Loss: 0.6763\n",
      "Epoch [92/100] complete, Total Loss: 0.6945\n",
      "Epoch [93/100] complete, Total Loss: 0.6718\n",
      "Epoch [94/100] complete, Total Loss: 0.6741\n",
      "Epoch [95/100] complete, Total Loss: 0.6586\n",
      "Epoch [96/100] complete, Total Loss: 0.6573\n",
      "Epoch [97/100] complete, Total Loss: 0.6770\n",
      "Epoch [98/100] complete, Total Loss: 0.6577\n",
      "Epoch [99/100] complete, Total Loss: 0.6483\n",
      "Epoch [100/100] complete, Total Loss: 0.6379\n",
      "CNN training complete.\n",
      "CNN model and weights saved.\n",
      "Evaluating CNN on test set...\n",
      "\n",
      "Test Set Evaluation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40         4\n",
      "           1       0.87      1.00      0.93        26\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.87        31\n",
      "   macro avg       0.62      0.42      0.44        31\n",
      "weighted avg       0.86      0.87      0.83        31\n",
      "\n",
      "Accuracy: 0.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaron/GitHub/bespoke/hackathon-warmup/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/aaron/GitHub/bespoke/hackathon-warmup/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/aaron/GitHub/bespoke/hackathon-warmup/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read\n",
    "from scipy import signal\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Set device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Function to apply padding and create a mask\n",
    "def pad_sequences(sequences, max_len=None, padding_value=0):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "    padded_seqs = np.full((len(sequences), max_len), padding_value, dtype=np.float32)\n",
    "    masks = np.zeros((len(sequences), max_len), dtype=np.float32)\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = len(seq)\n",
    "        padded_seqs[i, :seq_len] = seq\n",
    "        masks[i, :seq_len] = 1  # Valid data points\n",
    "\n",
    "    return padded_seqs, masks\n",
    "\n",
    "# Bandpass filter for seismic data\n",
    "def apply_bandpass_filter(trace, lowcut=0.5, highcut=1.0, sampling_rate=6.625, order=4):\n",
    "    sos = signal.butter(order, [lowcut, highcut], btype='bandpass', fs=sampling_rate, output='sos')\n",
    "    filtered_trace = signal.sosfilt(sos, trace)\n",
    "    return filtered_trace\n",
    "\n",
    "# STA/LTA feature extraction\n",
    "def extract_sta_lta_features(trace, sampling_rate, sta_window=1.0, lta_window=5.0, fixed_length=500):\n",
    "    sta_samples = int(sta_window * sampling_rate)\n",
    "    lta_samples = int(lta_window * sampling_rate)\n",
    "    cft = classic_sta_lta(trace, sta_samples, lta_samples)\n",
    "    \n",
    "    if len(cft) > fixed_length:\n",
    "        features = cft[:fixed_length]  # Truncate if longer\n",
    "    else:\n",
    "        features = np.pad(cft, (0, fixed_length - len(cft)), 'constant')  # Pad with zeros if shorter\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Complete preprocessing function\n",
    "def preprocess_seismic_data(filepath, filetype, sampling_rate=6.625):\n",
    "    if filetype == 'csv':\n",
    "        seismic_data = pd.read_csv(filepath)\n",
    "        trace = seismic_data['velocity(m/s)'].values\n",
    "    elif filetype == 'mseed':\n",
    "        st = read(filepath)\n",
    "        trace = st[0].data\n",
    "    \n",
    "    filtered_trace = apply_bandpass_filter(trace, sampling_rate=sampling_rate)\n",
    "    features = extract_sta_lta_features(filtered_trace, sampling_rate)\n",
    "    return features\n",
    "\n",
    "def load_seismic_data(data_dir, catalog_df=None, include_catalog=False):\n",
    "    seismic_data = []\n",
    "    labels = []\n",
    "    \n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            if file.endswith('.mseed'):\n",
    "                filetype = 'mseed'\n",
    "            elif file.endswith('.csv'):\n",
    "                filetype = 'csv'\n",
    "            else:\n",
    "                continue  # Skip unsupported file types\n",
    "            \n",
    "            # Preprocess seismic data (bandpass filtering and STA/LTA)\n",
    "            features = preprocess_seismic_data(filepath, filetype)\n",
    "            seismic_data.append(features)\n",
    "            \n",
    "            if include_catalog and catalog_df is not None:\n",
    "                event_id = os.path.splitext(file)[0]\n",
    "                if event_id in catalog_df['filename'].values:\n",
    "                    label_row = catalog_df.loc[catalog_df['filename'] == event_id]\n",
    "                    labels.append(label_row['mq_type'].values[0])  # Extract the string label\n",
    "    \n",
    "    # Convert seismic data to NumPy array\n",
    "    padded_data, masks = pad_sequences(seismic_data)\n",
    "\n",
    "    if include_catalog:\n",
    "        # Encode labels to numeric values\n",
    "        label_encoder = LabelEncoder()\n",
    "        labels_encoded = label_encoder.fit_transform(labels)  # Convert labels to integers\n",
    "        return padded_data, labels_encoded, masks\n",
    "    else:\n",
    "        return padded_data, masks\n",
    "\n",
    "class SeismicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SeismicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # After conv1 + pool\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # After conv2 + pool\n",
    "\n",
    "        # Flatten the output of the conv layers to pass to fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output dynamically based on batch size\n",
    "        \n",
    "        if self.fc1 is None:\n",
    "            # Dynamically initialize the fully connected layers based on the input size\n",
    "            self.fc1 = nn.Linear(x.size(1), 100)  # Use the computed flattened size\n",
    "            self.fc2 = nn.Linear(100, 3)  # For 3 classes\n",
    "\n",
    "        x = torch.relu(self.fc1(x))  # Pass through fully connected layer\n",
    "        x = self.fc2(x)  # Output layer (logits for 3 classes)\n",
    "        return x\n",
    "\n",
    "# Prepare data for PyTorch (ensure correct shape)\n",
    "def prepare_data_for_pytorch(X, y):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # Add channel dimension (batch_size, channels, features)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)  # Long tensor for labels\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "# Train CNN model for multi-class classification\n",
    "def train_cnn_model(train_loader, val_loader, num_epochs=100):\n",
    "    model = SeismicCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)  # Logits output\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] complete, Total Loss: {running_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "\n",
    "    print(\"\\nTest Set Evaluation Metrics:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    print(f\"Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "def main():\n",
    "    catalog_path = '../../data/lunar_data/training/catalogs/apollo12_catalog_GradeA_final.csv'\n",
    "    data_directory = '../../data/lunar_data/training/data/'\n",
    "\n",
    "    # Load and preprocess data\n",
    "    print(\"Preprocessing data...\")\n",
    "    catalog = pd.read_csv(catalog_path)\n",
    "    X, y, _ = load_seismic_data(data_directory, catalog_df=catalog, include_catalog=True)\n",
    "\n",
    "    # Train/Validation/Test split (60% train, 20% validation, 20% test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "    print(f\"Shape of training data: {X_train.shape}, Validation data: {X_val.shape}, Test data: {X_test.shape}\")\n",
    "\n",
    "    # Prepare data for PyTorch\n",
    "    train_loader = prepare_data_for_pytorch(X_train, y_train)\n",
    "    val_loader = prepare_data_for_pytorch(X_val, y_val)\n",
    "    test_loader = prepare_data_for_pytorch(X_test, y_test)\n",
    "\n",
    "    # Train CNN\n",
    "    print(\"Training CNN...\")\n",
    "    cnn_model = train_cnn_model(train_loader, val_loader, num_epochs=100)\n",
    "    print(\"CNN training complete.\")\n",
    "\n",
    "    # Save model and weights\n",
    "    torch.save({\n",
    "        'model_state_dict': cnn_model.state_dict(),\n",
    "        'model': cnn_model,\n",
    "    }, 'seismic_cnn_model.pth')\n",
    "    print(\"CNN model and weights saved.\")\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    print(\"Evaluating CNN on test set...\")\n",
    "    evaluate_model(cnn_model, test_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakdown of the Code:\n",
    "\n",
    "    Data Preprocessing:\n",
    "        Bandpass Filter: A bandpass filter is applied to the seismic traces to isolate frequency components between 0.5 and 1.0 Hz.\n",
    "        STA/LTA Feature Extraction: The classic STA/LTA algorithm is used to detect changes in signal amplitude, which can highlight seismic events. The resulting STA/LTA feature vector is truncated or padded to a fixed length (500).\n",
    "\n",
    "    Data Loading:\n",
    "        The seismic data is loaded from the directory, either in .mseed or .csv format, and then preprocessed (bandpass filtering and STA/LTA). Event labels are matched with the seismic data using the catalog file.\n",
    "\n",
    "    CNN Architecture:\n",
    "        The CNN uses two 1D convolutional layers followed by max-pooling layers. The output is flattened and passed through a fully connected layer for classification into three classes (Class 0, Class 1, Class 2).\n",
    "        The fully connected layer dimensions are dynamically set based on the flattened output size.\n",
    "\n",
    "    Training:\n",
    "        A typical training loop is used where the model is optimized using the Adam optimizer and CrossEntropy loss for 100 epochs. The loss decreases steadily, indicating learning progress.\n",
    "\n",
    "    Evaluation:\n",
    "        After training, the model is evaluated on a test set. Metrics such as precision, recall, F1-score, and accuracy are calculated to assess the model's performance.\n",
    "\n",
    "Analysis of Results:\n",
    "\n",
    "    Training Loss: The loss starts at 3.3378 and gradually decreases to 0.8191 over 100 epochs. This shows that the model is learning and adjusting its parameters during training.\n",
    "\n",
    "    Test Performance:\n",
    "\n",
    "        Class Imbalance:\n",
    "            The dataset is heavily imbalanced, as seen from the support values in the classification report:\n",
    "                Class 0 has 4 samples.\n",
    "                Class 1 has 26 samples.\n",
    "                Class 2 has 1 sample.\n",
    "            This imbalance can heavily skew the training process, causing the model to overfit to the majority class (Class 1), while performing poorly on the minority classes (Class 0 and Class 2).\n",
    "\n",
    "        Accuracy and Recall:\n",
    "            The accuracy of 84% is driven primarily by the model’s performance on Class 1, which has 26 samples, and the model correctly predicts all of them (recall of 1.00 for Class 1). However, this is misleading as it masks the poor performance on Class 0 and Class 2.\n",
    "\n",
    "        Zero Precision/Recall for Minority Classes (Class 0 and Class 2):\n",
    "            The model fails to correctly classify any samples from Class 0 and Class 2, with zero precision, recall, and F1-score for these classes. This is likely due to the class imbalance and the insufficient number of samples from these classes for the model to learn meaningful patterns.\n",
    "\n",
    "Key Issues and Recommendations:\n",
    "\n",
    "    Class Imbalance:\n",
    "        The main issue is the severe class imbalance, which causes the model to prioritize the majority class (Class 1) and ignore the minority classes (Class 0 and Class 2).\n",
    "        Solution: You can address this imbalance by using techniques such as:\n",
    "            Oversampling: Increase the number of samples in the minority classes by duplicating or generating synthetic data (e.g., using SMOTE).\n",
    "            Class Weights: Adjust the loss function to penalize misclassifications of minority classes more heavily by setting class weights in the CrossEntropyLoss function.\n",
    "\n",
    "    Limited Minority Class Data:\n",
    "        With only 1 sample in Class 2 and 4 samples in Class 0, the model has little opportunity to learn patterns for these classes.\n",
    "        Solution: It may be necessary to collect more data for these classes or apply data augmentation techniques to create more varied samples.\n",
    "\n",
    "    Model Evaluation:\n",
    "        Accuracy is not a reliable metric when dealing with imbalanced data, as it can be skewed by the majority class performance.\n",
    "        Solution: Focus on precision, recall, and F1-score for each class to get a better understanding of how well the model performs across all classes.\n",
    "\n",
    "    Dynamic Model Structure:\n",
    "        The model structure dynamically adapts to the input size, which is a flexible design. However, this dynamic initialization might not always work well when scaling to larger datasets or more complex architectures. A more predefined architecture can provide stability in training.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "The CNN model shows potential in classifying seismic events, but its performance is severely limited by the class imbalance in the dataset. To improve performance, especially on minority classes, you should apply class-balancing techniques such as oversampling, adding class weights to the loss function, or collecting more data for the minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
