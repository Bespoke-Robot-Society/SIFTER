{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from obspy import read\n",
    "from scipy import signal\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply padding and create a mask\n",
    "def pad_sequences(sequences, max_len=None, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad each sequence to the maximum length with a specified padding value and create a mask.\n",
    "    \"\"\"\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "    padded_seqs = np.full((len(sequences), max_len), padding_value, dtype=np.float32)\n",
    "    masks = np.zeros((len(sequences), max_len), dtype=np.float32)\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = len(seq)\n",
    "        padded_seqs[i, :seq_len] = seq\n",
    "        masks[i, :seq_len] = 1  # Valid data points\n",
    "\n",
    "    return padded_seqs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandpass filter for seismic data\n",
    "def apply_bandpass_filter(trace, lowcut=0.5, highcut=1.0, sampling_rate=6.625, order=4):\n",
    "    sos = signal.butter(order, [lowcut, highcut], btype='bandpass', fs=sampling_rate, output='sos')\n",
    "    filtered_trace = signal.sosfilt(sos, trace)\n",
    "    return filtered_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STA/LTA feature extraction\n",
    "def extract_sta_lta_features(trace, sampling_rate, sta_window=1.0, lta_window=5.0, fixed_length=500):\n",
    "    sta_samples = int(sta_window * sampling_rate)\n",
    "    lta_samples = int(lta_window * sampling_rate)\n",
    "    cft = classic_sta_lta(trace, sta_samples, lta_samples)\n",
    "    \n",
    "    if len(cft) > fixed_length:\n",
    "        features = cft[:fixed_length]  # Truncate if longer\n",
    "    else:\n",
    "        features = np.pad(cft, (0, fixed_length - len(cft)), 'constant')  # Pad with zeros if shorter\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete preprocessing function\n",
    "def preprocess_seismic_data(filepath, filetype, sampling_rate=6.625):\n",
    "    if filetype == 'csv':\n",
    "        seismic_data = pd.read_csv(filepath)\n",
    "        trace = seismic_data['velocity(m/s)'].values\n",
    "    elif filetype == 'mseed':\n",
    "        st = read(filepath)\n",
    "        trace = st[0].data\n",
    "    \n",
    "    filtered_trace = apply_bandpass_filter(trace, sampling_rate=sampling_rate)\n",
    "    features = extract_sta_lta_features(filtered_trace, sampling_rate)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seismic_data(data_dir, catalog_df=None, include_catalog=False):\n",
    "    seismic_data = []\n",
    "    labels = []\n",
    "    \n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            if file.endswith('.mseed'):\n",
    "                filetype = 'mseed'\n",
    "            elif file.endswith('.csv'):\n",
    "                filetype = 'csv'\n",
    "            else:\n",
    "                continue  # Skip unsupported file types\n",
    "            \n",
    "            # Preprocess seismic data (bandpass filtering and STA/LTA)\n",
    "            features = preprocess_seismic_data(filepath, filetype)\n",
    "            seismic_data.append(features)\n",
    "            \n",
    "            if include_catalog and catalog_df is not None:\n",
    "                event_id = os.path.splitext(file)[0]\n",
    "                if event_id in catalog_df['filename'].values:\n",
    "                    label_row = catalog_df.loc[catalog_df['filename'] == event_id]\n",
    "                    labels.append(label_row['mq_type'].values[0])  # Extract the string label\n",
    "    \n",
    "    # Convert seismic data to NumPy array\n",
    "    padded_data, masks = pad_sequences(seismic_data)\n",
    "\n",
    "    if include_catalog:\n",
    "        # Encode labels to numeric values\n",
    "        label_encoder = LabelEncoder()\n",
    "        labels_encoded = label_encoder.fit_transform(labels)  # Convert labels to integers\n",
    "        return padded_data, labels_encoded, masks\n",
    "    else:\n",
    "        return padded_data, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeismicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SeismicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Placeholder for fully connected layer input size (to be calculated dynamically)\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # After conv1 + pool\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # After conv2 + pool\n",
    "\n",
    "        # Flatten the output of the conv layers to pass to fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output dynamically based on batch size\n",
    "        \n",
    "        if self.fc1 is None:\n",
    "            # Dynamically initialize the fully connected layers based on the input size\n",
    "            self.fc1 = nn.Linear(x.size(1), 100)  # Use the computed flattened size\n",
    "            self.fc2 = nn.Linear(100, 3)  # For 3 classes\n",
    "\n",
    "        x = torch.relu(self.fc1(x))  # Pass through fully connected layer\n",
    "        x = self.fc2(x)  # Output layer (logits for 3 classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch (ensure correct shape)\n",
    "def prepare_data_for_pytorch(X, y):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # Add channel dimension (batch_size, channels, features)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)  # Long tensor for labels\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN model for multi-class classification\n",
    "def train_cnn_model(train_loader, num_epochs=100):\n",
    "    model = SeismicCNN()\n",
    "    criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)  # Logits output\n",
    "\n",
    "            # Calculate loss (CrossEntropyLoss expects raw logits and integer labels)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] complete, Total Loss: {running_loss:.4f}\")\n",
    "\n",
    "    return model  # Return the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and weights\n",
    "def save_model_artifacts(model, model_name='seismic_cnn_model'):\n",
    "    # 1. Save the model architecture (artifact) in a JSON or similar format\n",
    "    model_architecture = {\n",
    "        'input_size': model.conv1.in_channels,\n",
    "        'conv_layers': [\n",
    "            {'in_channels': model.conv1.in_channels, 'out_channels': model.conv1.out_channels, 'kernel_size': model.conv1.kernel_size, 'stride': model.conv1.stride, 'padding': model.conv1.padding},\n",
    "            {'in_channels': model.conv2.in_channels, 'out_channels': model.conv2.out_channels, 'kernel_size': model.conv2.kernel_size, 'stride': model.conv2.stride, 'padding': model.conv2.padding}\n",
    "        ],\n",
    "        'fc_layers': [\n",
    "            {'in_features': model.fc1.in_features, 'out_features': model.fc1.out_features},\n",
    "            {'in_features': model.fc2.in_features, 'out_features': model.fc2.out_features}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(f'{model_name}_architecture.json', 'w') as f:\n",
    "        json.dump(model_architecture, f)\n",
    "    print(f\"Model architecture saved to {model_name}_architecture.json\")\n",
    "\n",
    "    # 2. Save the model weights\n",
    "    torch.save(model.state_dict(), f'{model_name}_weights.pth')\n",
    "    print(f\"Model weights saved to {model_name}_weights.pth\")\n",
    "\n",
    "    # 3. Save the full model (model + weights)\n",
    "    torch.save(model, f'{model_name}_full.pth')\n",
    "    print(f\"Full model saved to {model_name}_full.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    catalog_path = '../../data/lunar_data/training/catalogs/apollo12_catalog_GradeA_final.csv'\n",
    "    data_directory = '../../data/lunar_data/training/data/'\n",
    "\n",
    "    # Load and preprocess data\n",
    "    print(\"Preprocessing data...\")\n",
    "    catalog = pd.read_csv(catalog_path)\n",
    "    X, y, _ = load_seismic_data(data_directory, catalog_df=catalog, include_catalog=True)\n",
    "\n",
    "    print(f\"Shape of features (X): {X.shape}\")\n",
    "    print(f\"Shape of labels (y): {y.shape}\")\n",
    "    print(f\"Unique labels: {np.unique(y)}\")\n",
    "\n",
    "    if X.shape[0] == 0 or y.shape[0] == 0:\n",
    "        print(\"No data to train on!\")\n",
    "        return\n",
    "\n",
    "    # Train CNN\n",
    "    print(\"Training CNN...\")\n",
    "    train_loader = prepare_data_for_pytorch(X, y)\n",
    "    cnn_model = train_cnn_model(train_loader)  # Get the trained model\n",
    "    print(\"CNN training complete.\")\n",
    "\n",
    "    # Save model and weights\n",
    "    save_model_artifacts(cnn_model, 'seismic_cnn_model')  # Pass the model to the save function\n",
    "    print(\"CNN model and weights saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Shape of features (X): (152, 500)\n",
      "Shape of labels (y): (152,)\n",
      "Unique labels: [0 1 2]\n",
      "Training CNN...\n",
      "Epoch [1/100], Batch [1/5], Loss: 1.0302\n",
      "Epoch [1/100], Batch [2/5], Loss: 0.9859\n",
      "Epoch [1/100], Batch [3/5], Loss: 0.9427\n",
      "Epoch [1/100], Batch [4/5], Loss: 0.9016\n",
      "Epoch [1/100], Batch [5/5], Loss: 0.8304\n",
      "Epoch [1/100] complete, Total Loss: 4.6908\n",
      "Epoch [2/100], Batch [1/5], Loss: 0.8788\n",
      "Epoch [2/100], Batch [2/5], Loss: 0.8180\n",
      "Epoch [2/100], Batch [3/5], Loss: 0.7749\n",
      "Epoch [2/100], Batch [4/5], Loss: 0.7062\n",
      "Epoch [2/100], Batch [5/5], Loss: 0.7194\n",
      "Epoch [2/100] complete, Total Loss: 3.8972\n",
      "Epoch [3/100], Batch [1/5], Loss: 0.7102\n",
      "Epoch [3/100], Batch [2/5], Loss: 0.6879\n",
      "Epoch [3/100], Batch [3/5], Loss: 0.7090\n",
      "Epoch [3/100], Batch [4/5], Loss: 0.6688\n",
      "Epoch [3/100], Batch [5/5], Loss: 0.5862\n",
      "Epoch [3/100] complete, Total Loss: 3.3620\n",
      "Epoch [4/100], Batch [1/5], Loss: 0.5825\n",
      "Epoch [4/100], Batch [2/5], Loss: 0.6672\n",
      "Epoch [4/100], Batch [3/5], Loss: 0.5887\n",
      "Epoch [4/100], Batch [4/5], Loss: 0.5970\n",
      "Epoch [4/100], Batch [5/5], Loss: 0.6001\n",
      "Epoch [4/100] complete, Total Loss: 3.0353\n",
      "Epoch [5/100], Batch [1/5], Loss: 0.5873\n",
      "Epoch [5/100], Batch [2/5], Loss: 0.4422\n",
      "Epoch [5/100], Batch [3/5], Loss: 0.5068\n",
      "Epoch [5/100], Batch [4/5], Loss: 0.7925\n",
      "Epoch [5/100], Batch [5/5], Loss: 0.4965\n",
      "Epoch [5/100] complete, Total Loss: 2.8253\n",
      "Epoch [6/100], Batch [1/5], Loss: 0.5009\n",
      "Epoch [6/100], Batch [2/5], Loss: 0.3446\n",
      "Epoch [6/100], Batch [3/5], Loss: 0.5944\n",
      "Epoch [6/100], Batch [4/5], Loss: 0.7486\n",
      "Epoch [6/100], Batch [5/5], Loss: 0.5631\n",
      "Epoch [6/100] complete, Total Loss: 2.7515\n",
      "Epoch [7/100], Batch [1/5], Loss: 0.7210\n",
      "Epoch [7/100], Batch [2/5], Loss: 0.3973\n",
      "Epoch [7/100], Batch [3/5], Loss: 0.5514\n",
      "Epoch [7/100], Batch [4/5], Loss: 0.3150\n",
      "Epoch [7/100], Batch [5/5], Loss: 0.7578\n",
      "Epoch [7/100] complete, Total Loss: 2.7424\n",
      "Epoch [8/100], Batch [1/5], Loss: 0.6592\n",
      "Epoch [8/100], Batch [2/5], Loss: 0.3471\n",
      "Epoch [8/100], Batch [3/5], Loss: 0.5052\n",
      "Epoch [8/100], Batch [4/5], Loss: 0.6248\n",
      "Epoch [8/100], Batch [5/5], Loss: 0.5182\n",
      "Epoch [8/100] complete, Total Loss: 2.6544\n",
      "Epoch [9/100], Batch [1/5], Loss: 0.5512\n",
      "Epoch [9/100], Batch [2/5], Loss: 0.6833\n",
      "Epoch [9/100], Batch [3/5], Loss: 0.6391\n",
      "Epoch [9/100], Batch [4/5], Loss: 0.2383\n",
      "Epoch [9/100], Batch [5/5], Loss: 0.5160\n",
      "Epoch [9/100] complete, Total Loss: 2.6278\n",
      "Epoch [10/100], Batch [1/5], Loss: 0.5211\n",
      "Epoch [10/100], Batch [2/5], Loss: 0.3762\n",
      "Epoch [10/100], Batch [3/5], Loss: 0.6885\n",
      "Epoch [10/100], Batch [4/5], Loss: 0.5659\n",
      "Epoch [10/100], Batch [5/5], Loss: 0.4193\n",
      "Epoch [10/100] complete, Total Loss: 2.5710\n",
      "Epoch [11/100], Batch [1/5], Loss: 0.4349\n",
      "Epoch [11/100], Batch [2/5], Loss: 0.4549\n",
      "Epoch [11/100], Batch [3/5], Loss: 0.6183\n",
      "Epoch [11/100], Batch [4/5], Loss: 0.4479\n",
      "Epoch [11/100], Batch [5/5], Loss: 0.6416\n",
      "Epoch [11/100] complete, Total Loss: 2.5977\n",
      "Epoch [12/100], Batch [1/5], Loss: 0.8161\n",
      "Epoch [12/100], Batch [2/5], Loss: 0.2678\n",
      "Epoch [12/100], Batch [3/5], Loss: 0.5667\n",
      "Epoch [12/100], Batch [4/5], Loss: 0.3007\n",
      "Epoch [12/100], Batch [5/5], Loss: 0.6160\n",
      "Epoch [12/100] complete, Total Loss: 2.5673\n",
      "Epoch [13/100], Batch [1/5], Loss: 0.6771\n",
      "Epoch [13/100], Batch [2/5], Loss: 0.5585\n",
      "Epoch [13/100], Batch [3/5], Loss: 0.5078\n",
      "Epoch [13/100], Batch [4/5], Loss: 0.3555\n",
      "Epoch [13/100], Batch [5/5], Loss: 0.3804\n",
      "Epoch [13/100] complete, Total Loss: 2.4794\n",
      "Epoch [14/100], Batch [1/5], Loss: 0.4807\n",
      "Epoch [14/100], Batch [2/5], Loss: 0.4763\n",
      "Epoch [14/100], Batch [3/5], Loss: 0.3867\n",
      "Epoch [14/100], Batch [4/5], Loss: 0.6176\n",
      "Epoch [14/100], Batch [5/5], Loss: 0.5266\n",
      "Epoch [14/100] complete, Total Loss: 2.4879\n",
      "Epoch [15/100], Batch [1/5], Loss: 0.3754\n",
      "Epoch [15/100], Batch [2/5], Loss: 0.6358\n",
      "Epoch [15/100], Batch [3/5], Loss: 0.5097\n",
      "Epoch [15/100], Batch [4/5], Loss: 0.4391\n",
      "Epoch [15/100], Batch [5/5], Loss: 0.4955\n",
      "Epoch [15/100] complete, Total Loss: 2.4555\n",
      "Epoch [16/100], Batch [1/5], Loss: 0.5817\n",
      "Epoch [16/100], Batch [2/5], Loss: 0.3413\n",
      "Epoch [16/100], Batch [3/5], Loss: 0.4621\n",
      "Epoch [16/100], Batch [4/5], Loss: 0.5663\n",
      "Epoch [16/100], Batch [5/5], Loss: 0.4748\n",
      "Epoch [16/100] complete, Total Loss: 2.4262\n",
      "Epoch [17/100], Batch [1/5], Loss: 0.4280\n",
      "Epoch [17/100], Batch [2/5], Loss: 0.7960\n",
      "Epoch [17/100], Batch [3/5], Loss: 0.4096\n",
      "Epoch [17/100], Batch [4/5], Loss: 0.4803\n",
      "Epoch [17/100], Batch [5/5], Loss: 0.2276\n",
      "Epoch [17/100] complete, Total Loss: 2.3413\n",
      "Epoch [18/100], Batch [1/5], Loss: 0.4724\n",
      "Epoch [18/100], Batch [2/5], Loss: 0.4061\n",
      "Epoch [18/100], Batch [3/5], Loss: 0.4502\n",
      "Epoch [18/100], Batch [4/5], Loss: 0.6501\n",
      "Epoch [18/100], Batch [5/5], Loss: 0.3807\n",
      "Epoch [18/100] complete, Total Loss: 2.3595\n",
      "Epoch [19/100], Batch [1/5], Loss: 0.3347\n",
      "Epoch [19/100], Batch [2/5], Loss: 0.3981\n",
      "Epoch [19/100], Batch [3/5], Loss: 0.5049\n",
      "Epoch [19/100], Batch [4/5], Loss: 0.4867\n",
      "Epoch [19/100], Batch [5/5], Loss: 0.6932\n",
      "Epoch [19/100] complete, Total Loss: 2.4177\n",
      "Epoch [20/100], Batch [1/5], Loss: 0.3252\n",
      "Epoch [20/100], Batch [2/5], Loss: 0.6057\n",
      "Epoch [20/100], Batch [3/5], Loss: 0.5076\n",
      "Epoch [20/100], Batch [4/5], Loss: 0.4252\n",
      "Epoch [20/100], Batch [5/5], Loss: 0.4793\n",
      "Epoch [20/100] complete, Total Loss: 2.3431\n",
      "Epoch [21/100], Batch [1/5], Loss: 0.6062\n",
      "Epoch [21/100], Batch [2/5], Loss: 0.4170\n",
      "Epoch [21/100], Batch [3/5], Loss: 0.4230\n",
      "Epoch [21/100], Batch [4/5], Loss: 0.6198\n",
      "Epoch [21/100], Batch [5/5], Loss: 0.1866\n",
      "Epoch [21/100] complete, Total Loss: 2.2526\n",
      "Epoch [22/100], Batch [1/5], Loss: 0.4043\n",
      "Epoch [22/100], Batch [2/5], Loss: 0.5945\n",
      "Epoch [22/100], Batch [3/5], Loss: 0.3982\n",
      "Epoch [22/100], Batch [4/5], Loss: 0.5362\n",
      "Epoch [22/100], Batch [5/5], Loss: 0.3367\n",
      "Epoch [22/100] complete, Total Loss: 2.2700\n",
      "Epoch [23/100], Batch [1/5], Loss: 0.3061\n",
      "Epoch [23/100], Batch [2/5], Loss: 0.2682\n",
      "Epoch [23/100], Batch [3/5], Loss: 0.4287\n",
      "Epoch [23/100], Batch [4/5], Loss: 0.5381\n",
      "Epoch [23/100], Batch [5/5], Loss: 0.8457\n",
      "Epoch [23/100] complete, Total Loss: 2.3868\n",
      "Epoch [24/100], Batch [1/5], Loss: 0.3545\n",
      "Epoch [24/100], Batch [2/5], Loss: 0.3425\n",
      "Epoch [24/100], Batch [3/5], Loss: 0.2973\n",
      "Epoch [24/100], Batch [4/5], Loss: 0.5515\n",
      "Epoch [24/100], Batch [5/5], Loss: 0.8072\n",
      "Epoch [24/100] complete, Total Loss: 2.3530\n",
      "Epoch [25/100], Batch [1/5], Loss: 0.5610\n",
      "Epoch [25/100], Batch [2/5], Loss: 0.4583\n",
      "Epoch [25/100], Batch [3/5], Loss: 0.4278\n",
      "Epoch [25/100], Batch [4/5], Loss: 0.3419\n",
      "Epoch [25/100], Batch [5/5], Loss: 0.4602\n",
      "Epoch [25/100] complete, Total Loss: 2.2491\n",
      "Epoch [26/100], Batch [1/5], Loss: 0.2516\n",
      "Epoch [26/100], Batch [2/5], Loss: 0.5563\n",
      "Epoch [26/100], Batch [3/5], Loss: 0.4840\n",
      "Epoch [26/100], Batch [4/5], Loss: 0.4135\n",
      "Epoch [26/100], Batch [5/5], Loss: 0.5470\n",
      "Epoch [26/100] complete, Total Loss: 2.2525\n",
      "Epoch [27/100], Batch [1/5], Loss: 0.5893\n",
      "Epoch [27/100], Batch [2/5], Loss: 0.4815\n",
      "Epoch [27/100], Batch [3/5], Loss: 0.2339\n",
      "Epoch [27/100], Batch [4/5], Loss: 0.4836\n",
      "Epoch [27/100], Batch [5/5], Loss: 0.4163\n",
      "Epoch [27/100] complete, Total Loss: 2.2045\n",
      "Epoch [28/100], Batch [1/5], Loss: 0.5970\n",
      "Epoch [28/100], Batch [2/5], Loss: 0.2028\n",
      "Epoch [28/100], Batch [3/5], Loss: 0.5747\n",
      "Epoch [28/100], Batch [4/5], Loss: 0.3158\n",
      "Epoch [28/100], Batch [5/5], Loss: 0.5222\n",
      "Epoch [28/100] complete, Total Loss: 2.2125\n",
      "Epoch [29/100], Batch [1/5], Loss: 0.3174\n",
      "Epoch [29/100], Batch [2/5], Loss: 0.3653\n",
      "Epoch [29/100], Batch [3/5], Loss: 0.5369\n",
      "Epoch [29/100], Batch [4/5], Loss: 0.6705\n",
      "Epoch [29/100], Batch [5/5], Loss: 0.2347\n",
      "Epoch [29/100] complete, Total Loss: 2.1247\n",
      "Epoch [30/100], Batch [1/5], Loss: 0.3777\n",
      "Epoch [30/100], Batch [2/5], Loss: 0.3786\n",
      "Epoch [30/100], Batch [3/5], Loss: 0.4396\n",
      "Epoch [30/100], Batch [4/5], Loss: 0.4233\n",
      "Epoch [30/100], Batch [5/5], Loss: 0.5721\n",
      "Epoch [30/100] complete, Total Loss: 2.1913\n",
      "Epoch [31/100], Batch [1/5], Loss: 0.3738\n",
      "Epoch [31/100], Batch [2/5], Loss: 0.5385\n",
      "Epoch [31/100], Batch [3/5], Loss: 0.4232\n",
      "Epoch [31/100], Batch [4/5], Loss: 0.4958\n",
      "Epoch [31/100], Batch [5/5], Loss: 0.2617\n",
      "Epoch [31/100] complete, Total Loss: 2.0929\n",
      "Epoch [32/100], Batch [1/5], Loss: 0.2093\n",
      "Epoch [32/100], Batch [2/5], Loss: 0.6329\n",
      "Epoch [32/100], Batch [3/5], Loss: 0.5439\n",
      "Epoch [32/100], Batch [4/5], Loss: 0.3329\n",
      "Epoch [32/100], Batch [5/5], Loss: 0.3909\n",
      "Epoch [32/100] complete, Total Loss: 2.1099\n",
      "Epoch [33/100], Batch [1/5], Loss: 0.4500\n",
      "Epoch [33/100], Batch [2/5], Loss: 0.3837\n",
      "Epoch [33/100], Batch [3/5], Loss: 0.3821\n",
      "Epoch [33/100], Batch [4/5], Loss: 0.3816\n",
      "Epoch [33/100], Batch [5/5], Loss: 0.5283\n",
      "Epoch [33/100] complete, Total Loss: 2.1258\n",
      "Epoch [34/100], Batch [1/5], Loss: 0.4400\n",
      "Epoch [34/100], Batch [2/5], Loss: 0.4095\n",
      "Epoch [34/100], Batch [3/5], Loss: 0.2552\n",
      "Epoch [34/100], Batch [4/5], Loss: 0.6231\n",
      "Epoch [34/100], Batch [5/5], Loss: 0.3356\n",
      "Epoch [34/100] complete, Total Loss: 2.0635\n",
      "Epoch [35/100], Batch [1/5], Loss: 0.4416\n",
      "Epoch [35/100], Batch [2/5], Loss: 0.5217\n",
      "Epoch [35/100], Batch [3/5], Loss: 0.3395\n",
      "Epoch [35/100], Batch [4/5], Loss: 0.3555\n",
      "Epoch [35/100], Batch [5/5], Loss: 0.4054\n",
      "Epoch [35/100] complete, Total Loss: 2.0638\n",
      "Epoch [36/100], Batch [1/5], Loss: 0.1829\n",
      "Epoch [36/100], Batch [2/5], Loss: 0.3229\n",
      "Epoch [36/100], Batch [3/5], Loss: 0.6436\n",
      "Epoch [36/100], Batch [4/5], Loss: 0.3878\n",
      "Epoch [36/100], Batch [5/5], Loss: 0.5472\n",
      "Epoch [36/100] complete, Total Loss: 2.0844\n",
      "Epoch [37/100], Batch [1/5], Loss: 0.6043\n",
      "Epoch [37/100], Batch [2/5], Loss: 0.3262\n",
      "Epoch [37/100], Batch [3/5], Loss: 0.4672\n",
      "Epoch [37/100], Batch [4/5], Loss: 0.2455\n",
      "Epoch [37/100], Batch [5/5], Loss: 0.3892\n",
      "Epoch [37/100] complete, Total Loss: 2.0324\n",
      "Epoch [38/100], Batch [1/5], Loss: 0.4450\n",
      "Epoch [38/100], Batch [2/5], Loss: 0.3353\n",
      "Epoch [38/100], Batch [3/5], Loss: 0.4982\n",
      "Epoch [38/100], Batch [4/5], Loss: 0.3963\n",
      "Epoch [38/100], Batch [5/5], Loss: 0.3232\n",
      "Epoch [38/100] complete, Total Loss: 1.9980\n",
      "Epoch [39/100], Batch [1/5], Loss: 0.3789\n",
      "Epoch [39/100], Batch [2/5], Loss: 0.4386\n",
      "Epoch [39/100], Batch [3/5], Loss: 0.2850\n",
      "Epoch [39/100], Batch [4/5], Loss: 0.4298\n",
      "Epoch [39/100], Batch [5/5], Loss: 0.4904\n",
      "Epoch [39/100] complete, Total Loss: 2.0228\n",
      "Epoch [40/100], Batch [1/5], Loss: 0.4644\n",
      "Epoch [40/100], Batch [2/5], Loss: 0.3099\n",
      "Epoch [40/100], Batch [3/5], Loss: 0.2817\n",
      "Epoch [40/100], Batch [4/5], Loss: 0.6895\n",
      "Epoch [40/100], Batch [5/5], Loss: 0.1889\n",
      "Epoch [40/100] complete, Total Loss: 1.9343\n",
      "Epoch [41/100], Batch [1/5], Loss: 0.7466\n",
      "Epoch [41/100], Batch [2/5], Loss: 0.2281\n",
      "Epoch [41/100], Batch [3/5], Loss: 0.3967\n",
      "Epoch [41/100], Batch [4/5], Loss: 0.2631\n",
      "Epoch [41/100], Batch [5/5], Loss: 0.3189\n",
      "Epoch [41/100] complete, Total Loss: 1.9534\n",
      "Epoch [42/100], Batch [1/5], Loss: 0.2264\n",
      "Epoch [42/100], Batch [2/5], Loss: 0.3956\n",
      "Epoch [42/100], Batch [3/5], Loss: 0.2426\n",
      "Epoch [42/100], Batch [4/5], Loss: 0.6584\n",
      "Epoch [42/100], Batch [5/5], Loss: 0.4453\n",
      "Epoch [42/100] complete, Total Loss: 1.9684\n",
      "Epoch [43/100], Batch [1/5], Loss: 0.5433\n",
      "Epoch [43/100], Batch [2/5], Loss: 0.3565\n",
      "Epoch [43/100], Batch [3/5], Loss: 0.3370\n",
      "Epoch [43/100], Batch [4/5], Loss: 0.3519\n",
      "Epoch [43/100], Batch [5/5], Loss: 0.3341\n",
      "Epoch [43/100] complete, Total Loss: 1.9228\n",
      "Epoch [44/100], Batch [1/5], Loss: 0.2886\n",
      "Epoch [44/100], Batch [2/5], Loss: 0.3577\n",
      "Epoch [44/100], Batch [3/5], Loss: 0.4642\n",
      "Epoch [44/100], Batch [4/5], Loss: 0.4720\n",
      "Epoch [44/100], Batch [5/5], Loss: 0.3204\n",
      "Epoch [44/100] complete, Total Loss: 1.9029\n",
      "Epoch [45/100], Batch [1/5], Loss: 0.4340\n",
      "Epoch [45/100], Batch [2/5], Loss: 0.5025\n",
      "Epoch [45/100], Batch [3/5], Loss: 0.3103\n",
      "Epoch [45/100], Batch [4/5], Loss: 0.1908\n",
      "Epoch [45/100], Batch [5/5], Loss: 0.4962\n",
      "Epoch [45/100] complete, Total Loss: 1.9339\n",
      "Epoch [46/100], Batch [1/5], Loss: 0.3941\n",
      "Epoch [46/100], Batch [2/5], Loss: 0.3579\n",
      "Epoch [46/100], Batch [3/5], Loss: 0.3719\n",
      "Epoch [46/100], Batch [4/5], Loss: 0.4363\n",
      "Epoch [46/100], Batch [5/5], Loss: 0.3155\n",
      "Epoch [46/100] complete, Total Loss: 1.8757\n",
      "Epoch [47/100], Batch [1/5], Loss: 0.3814\n",
      "Epoch [47/100], Batch [2/5], Loss: 0.2916\n",
      "Epoch [47/100], Batch [3/5], Loss: 0.3434\n",
      "Epoch [47/100], Batch [4/5], Loss: 0.6279\n",
      "Epoch [47/100], Batch [5/5], Loss: 0.1821\n",
      "Epoch [47/100] complete, Total Loss: 1.8264\n",
      "Epoch [48/100], Batch [1/5], Loss: 0.3499\n",
      "Epoch [48/100], Batch [2/5], Loss: 0.3194\n",
      "Epoch [48/100], Batch [3/5], Loss: 0.2710\n",
      "Epoch [48/100], Batch [4/5], Loss: 0.5322\n",
      "Epoch [48/100], Batch [5/5], Loss: 0.3913\n",
      "Epoch [48/100] complete, Total Loss: 1.8637\n",
      "Epoch [49/100], Batch [1/5], Loss: 0.3981\n",
      "Epoch [49/100], Batch [2/5], Loss: 0.3057\n",
      "Epoch [49/100], Batch [3/5], Loss: 0.2918\n",
      "Epoch [49/100], Batch [4/5], Loss: 0.4602\n",
      "Epoch [49/100], Batch [5/5], Loss: 0.3950\n",
      "Epoch [49/100] complete, Total Loss: 1.8508\n",
      "Epoch [50/100], Batch [1/5], Loss: 0.3145\n",
      "Epoch [50/100], Batch [2/5], Loss: 0.3765\n",
      "Epoch [50/100], Batch [3/5], Loss: 0.4027\n",
      "Epoch [50/100], Batch [4/5], Loss: 0.3673\n",
      "Epoch [50/100], Batch [5/5], Loss: 0.3702\n",
      "Epoch [50/100] complete, Total Loss: 1.8312\n",
      "Epoch [51/100], Batch [1/5], Loss: 0.2856\n",
      "Epoch [51/100], Batch [2/5], Loss: 0.4944\n",
      "Epoch [51/100], Batch [3/5], Loss: 0.4623\n",
      "Epoch [51/100], Batch [4/5], Loss: 0.2456\n",
      "Epoch [51/100], Batch [5/5], Loss: 0.3172\n",
      "Epoch [51/100] complete, Total Loss: 1.8051\n",
      "Epoch [52/100], Batch [1/5], Loss: 0.2327\n",
      "Epoch [52/100], Batch [2/5], Loss: 0.4686\n",
      "Epoch [52/100], Batch [3/5], Loss: 0.2588\n",
      "Epoch [52/100], Batch [4/5], Loss: 0.3923\n",
      "Epoch [52/100], Batch [5/5], Loss: 0.4797\n",
      "Epoch [52/100] complete, Total Loss: 1.8320\n",
      "Epoch [53/100], Batch [1/5], Loss: 0.2930\n",
      "Epoch [53/100], Batch [2/5], Loss: 0.3643\n",
      "Epoch [53/100], Batch [3/5], Loss: 0.2780\n",
      "Epoch [53/100], Batch [4/5], Loss: 0.5000\n",
      "Epoch [53/100], Batch [5/5], Loss: 0.3470\n",
      "Epoch [53/100] complete, Total Loss: 1.7823\n",
      "Epoch [54/100], Batch [1/5], Loss: 0.4520\n",
      "Epoch [54/100], Batch [2/5], Loss: 0.4371\n",
      "Epoch [54/100], Batch [3/5], Loss: 0.1579\n",
      "Epoch [54/100], Batch [4/5], Loss: 0.4387\n",
      "Epoch [54/100], Batch [5/5], Loss: 0.2676\n",
      "Epoch [54/100] complete, Total Loss: 1.7533\n",
      "Epoch [55/100], Batch [1/5], Loss: 0.4598\n",
      "Epoch [55/100], Batch [2/5], Loss: 0.4051\n",
      "Epoch [55/100], Batch [3/5], Loss: 0.2199\n",
      "Epoch [55/100], Batch [4/5], Loss: 0.4249\n",
      "Epoch [55/100], Batch [5/5], Loss: 0.2208\n",
      "Epoch [55/100] complete, Total Loss: 1.7307\n",
      "Epoch [56/100], Batch [1/5], Loss: 0.4080\n",
      "Epoch [56/100], Batch [2/5], Loss: 0.2161\n",
      "Epoch [56/100], Batch [3/5], Loss: 0.3261\n",
      "Epoch [56/100], Batch [4/5], Loss: 0.5257\n",
      "Epoch [56/100], Batch [5/5], Loss: 0.2471\n",
      "Epoch [56/100] complete, Total Loss: 1.7231\n",
      "Epoch [57/100], Batch [1/5], Loss: 0.3841\n",
      "Epoch [57/100], Batch [2/5], Loss: 0.2245\n",
      "Epoch [57/100], Batch [3/5], Loss: 0.3482\n",
      "Epoch [57/100], Batch [4/5], Loss: 0.2566\n",
      "Epoch [57/100], Batch [5/5], Loss: 0.5831\n",
      "Epoch [57/100] complete, Total Loss: 1.7965\n",
      "Epoch [58/100], Batch [1/5], Loss: 0.3620\n",
      "Epoch [58/100], Batch [2/5], Loss: 0.3911\n",
      "Epoch [58/100], Batch [3/5], Loss: 0.5193\n",
      "Epoch [58/100], Batch [4/5], Loss: 0.1575\n",
      "Epoch [58/100], Batch [5/5], Loss: 0.2779\n",
      "Epoch [58/100] complete, Total Loss: 1.7079\n",
      "Epoch [59/100], Batch [1/5], Loss: 0.4687\n",
      "Epoch [59/100], Batch [2/5], Loss: 0.3585\n",
      "Epoch [59/100], Batch [3/5], Loss: 0.2619\n",
      "Epoch [59/100], Batch [4/5], Loss: 0.3546\n",
      "Epoch [59/100], Batch [5/5], Loss: 0.2444\n",
      "Epoch [59/100] complete, Total Loss: 1.6882\n",
      "Epoch [60/100], Batch [1/5], Loss: 0.2904\n",
      "Epoch [60/100], Batch [2/5], Loss: 0.3695\n",
      "Epoch [60/100], Batch [3/5], Loss: 0.3147\n",
      "Epoch [60/100], Batch [4/5], Loss: 0.3892\n",
      "Epoch [60/100], Batch [5/5], Loss: 0.3305\n",
      "Epoch [60/100] complete, Total Loss: 1.6943\n",
      "Epoch [61/100], Batch [1/5], Loss: 0.2272\n",
      "Epoch [61/100], Batch [2/5], Loss: 0.3950\n",
      "Epoch [61/100], Batch [3/5], Loss: 0.2852\n",
      "Epoch [61/100], Batch [4/5], Loss: 0.3796\n",
      "Epoch [61/100], Batch [5/5], Loss: 0.4161\n",
      "Epoch [61/100] complete, Total Loss: 1.7032\n",
      "Epoch [62/100], Batch [1/5], Loss: 0.3034\n",
      "Epoch [62/100], Batch [2/5], Loss: 0.3475\n",
      "Epoch [62/100], Batch [3/5], Loss: 0.2791\n",
      "Epoch [62/100], Batch [4/5], Loss: 0.3236\n",
      "Epoch [62/100], Batch [5/5], Loss: 0.4438\n",
      "Epoch [62/100] complete, Total Loss: 1.6974\n",
      "Epoch [63/100], Batch [1/5], Loss: 0.3035\n",
      "Epoch [63/100], Batch [2/5], Loss: 0.2948\n",
      "Epoch [63/100], Batch [3/5], Loss: 0.4492\n",
      "Epoch [63/100], Batch [4/5], Loss: 0.2189\n",
      "Epoch [63/100], Batch [5/5], Loss: 0.4106\n",
      "Epoch [63/100] complete, Total Loss: 1.6769\n",
      "Epoch [64/100], Batch [1/5], Loss: 0.4641\n",
      "Epoch [64/100], Batch [2/5], Loss: 0.2459\n",
      "Epoch [64/100], Batch [3/5], Loss: 0.3402\n",
      "Epoch [64/100], Batch [4/5], Loss: 0.3046\n",
      "Epoch [64/100], Batch [5/5], Loss: 0.2815\n",
      "Epoch [64/100] complete, Total Loss: 1.6362\n",
      "Epoch [65/100], Batch [1/5], Loss: 0.2749\n",
      "Epoch [65/100], Batch [2/5], Loss: 0.2990\n",
      "Epoch [65/100], Batch [3/5], Loss: 0.3326\n",
      "Epoch [65/100], Batch [4/5], Loss: 0.2462\n",
      "Epoch [65/100], Batch [5/5], Loss: 0.5242\n",
      "Epoch [65/100] complete, Total Loss: 1.6769\n",
      "Epoch [66/100], Batch [1/5], Loss: 0.1975\n",
      "Epoch [66/100], Batch [2/5], Loss: 0.4052\n",
      "Epoch [66/100], Batch [3/5], Loss: 0.4269\n",
      "Epoch [66/100], Batch [4/5], Loss: 0.2561\n",
      "Epoch [66/100], Batch [5/5], Loss: 0.3324\n",
      "Epoch [66/100] complete, Total Loss: 1.6181\n",
      "Epoch [67/100], Batch [1/5], Loss: 0.3022\n",
      "Epoch [67/100], Batch [2/5], Loss: 0.3711\n",
      "Epoch [67/100], Batch [3/5], Loss: 0.2095\n",
      "Epoch [67/100], Batch [4/5], Loss: 0.3123\n",
      "Epoch [67/100], Batch [5/5], Loss: 0.4353\n",
      "Epoch [67/100] complete, Total Loss: 1.6304\n",
      "Epoch [68/100], Batch [1/5], Loss: 0.2251\n",
      "Epoch [68/100], Batch [2/5], Loss: 0.3319\n",
      "Epoch [68/100], Batch [3/5], Loss: 0.3787\n",
      "Epoch [68/100], Batch [4/5], Loss: 0.2556\n",
      "Epoch [68/100], Batch [5/5], Loss: 0.4223\n",
      "Epoch [68/100] complete, Total Loss: 1.6137\n",
      "Epoch [69/100], Batch [1/5], Loss: 0.1281\n",
      "Epoch [69/100], Batch [2/5], Loss: 0.2482\n",
      "Epoch [69/100], Batch [3/5], Loss: 0.3351\n",
      "Epoch [69/100], Batch [4/5], Loss: 0.4266\n",
      "Epoch [69/100], Batch [5/5], Loss: 0.4823\n",
      "Epoch [69/100] complete, Total Loss: 1.6204\n",
      "Epoch [70/100], Batch [1/5], Loss: 0.2959\n",
      "Epoch [70/100], Batch [2/5], Loss: 0.2424\n",
      "Epoch [70/100], Batch [3/5], Loss: 0.1832\n",
      "Epoch [70/100], Batch [4/5], Loss: 0.4480\n",
      "Epoch [70/100], Batch [5/5], Loss: 0.4161\n",
      "Epoch [70/100] complete, Total Loss: 1.5856\n",
      "Epoch [71/100], Batch [1/5], Loss: 0.3400\n",
      "Epoch [71/100], Batch [2/5], Loss: 0.2705\n",
      "Epoch [71/100], Batch [3/5], Loss: 0.2845\n",
      "Epoch [71/100], Batch [4/5], Loss: 0.2962\n",
      "Epoch [71/100], Batch [5/5], Loss: 0.3705\n",
      "Epoch [71/100] complete, Total Loss: 1.5618\n",
      "Epoch [72/100], Batch [1/5], Loss: 0.1722\n",
      "Epoch [72/100], Batch [2/5], Loss: 0.3585\n",
      "Epoch [72/100], Batch [3/5], Loss: 0.2543\n",
      "Epoch [72/100], Batch [4/5], Loss: 0.4930\n",
      "Epoch [72/100], Batch [5/5], Loss: 0.2430\n",
      "Epoch [72/100] complete, Total Loss: 1.5211\n",
      "Epoch [73/100], Batch [1/5], Loss: 0.2480\n",
      "Epoch [73/100], Batch [2/5], Loss: 0.3336\n",
      "Epoch [73/100], Batch [3/5], Loss: 0.1996\n",
      "Epoch [73/100], Batch [4/5], Loss: 0.2502\n",
      "Epoch [73/100], Batch [5/5], Loss: 0.5587\n",
      "Epoch [73/100] complete, Total Loss: 1.5901\n",
      "Epoch [74/100], Batch [1/5], Loss: 0.3513\n",
      "Epoch [74/100], Batch [2/5], Loss: 0.4022\n",
      "Epoch [74/100], Batch [3/5], Loss: 0.2811\n",
      "Epoch [74/100], Batch [4/5], Loss: 0.1420\n",
      "Epoch [74/100], Batch [5/5], Loss: 0.3492\n",
      "Epoch [74/100] complete, Total Loss: 1.5257\n",
      "Epoch [75/100], Batch [1/5], Loss: 0.2809\n",
      "Epoch [75/100], Batch [2/5], Loss: 0.3218\n",
      "Epoch [75/100], Batch [3/5], Loss: 0.4133\n",
      "Epoch [75/100], Batch [4/5], Loss: 0.1070\n",
      "Epoch [75/100], Batch [5/5], Loss: 0.3918\n",
      "Epoch [75/100] complete, Total Loss: 1.5148\n",
      "Epoch [76/100], Batch [1/5], Loss: 0.2605\n",
      "Epoch [76/100], Batch [2/5], Loss: 0.2359\n",
      "Epoch [76/100], Batch [3/5], Loss: 0.3942\n",
      "Epoch [76/100], Batch [4/5], Loss: 0.3039\n",
      "Epoch [76/100], Batch [5/5], Loss: 0.2745\n",
      "Epoch [76/100] complete, Total Loss: 1.4690\n",
      "Epoch [77/100], Batch [1/5], Loss: 0.3623\n",
      "Epoch [77/100], Batch [2/5], Loss: 0.2980\n",
      "Epoch [77/100], Batch [3/5], Loss: 0.2105\n",
      "Epoch [77/100], Batch [4/5], Loss: 0.3456\n",
      "Epoch [77/100], Batch [5/5], Loss: 0.2257\n",
      "Epoch [77/100] complete, Total Loss: 1.4421\n",
      "Epoch [78/100], Batch [1/5], Loss: 0.2223\n",
      "Epoch [78/100], Batch [2/5], Loss: 0.3545\n",
      "Epoch [78/100], Batch [3/5], Loss: 0.3120\n",
      "Epoch [78/100], Batch [4/5], Loss: 0.2978\n",
      "Epoch [78/100], Batch [5/5], Loss: 0.2475\n",
      "Epoch [78/100] complete, Total Loss: 1.4340\n",
      "Epoch [79/100], Batch [1/5], Loss: 0.2856\n",
      "Epoch [79/100], Batch [2/5], Loss: 0.1835\n",
      "Epoch [79/100], Batch [3/5], Loss: 0.2864\n",
      "Epoch [79/100], Batch [4/5], Loss: 0.3808\n",
      "Epoch [79/100], Batch [5/5], Loss: 0.2949\n",
      "Epoch [79/100] complete, Total Loss: 1.4311\n",
      "Epoch [80/100], Batch [1/5], Loss: 0.2978\n",
      "Epoch [80/100], Batch [2/5], Loss: 0.3743\n",
      "Epoch [80/100], Batch [3/5], Loss: 0.2848\n",
      "Epoch [80/100], Batch [4/5], Loss: 0.1776\n",
      "Epoch [80/100], Batch [5/5], Loss: 0.2828\n",
      "Epoch [80/100] complete, Total Loss: 1.4174\n",
      "Epoch [81/100], Batch [1/5], Loss: 0.3255\n",
      "Epoch [81/100], Batch [2/5], Loss: 0.2213\n",
      "Epoch [81/100], Batch [3/5], Loss: 0.1993\n",
      "Epoch [81/100], Batch [4/5], Loss: 0.2486\n",
      "Epoch [81/100], Batch [5/5], Loss: 0.4525\n",
      "Epoch [81/100] complete, Total Loss: 1.4471\n",
      "Epoch [82/100], Batch [1/5], Loss: 0.2191\n",
      "Epoch [82/100], Batch [2/5], Loss: 0.3419\n",
      "Epoch [82/100], Batch [3/5], Loss: 0.2671\n",
      "Epoch [82/100], Batch [4/5], Loss: 0.2631\n",
      "Epoch [82/100], Batch [5/5], Loss: 0.2993\n",
      "Epoch [82/100] complete, Total Loss: 1.3905\n",
      "Epoch [83/100], Batch [1/5], Loss: 0.3185\n",
      "Epoch [83/100], Batch [2/5], Loss: 0.1981\n",
      "Epoch [83/100], Batch [3/5], Loss: 0.2641\n",
      "Epoch [83/100], Batch [4/5], Loss: 0.2393\n",
      "Epoch [83/100], Batch [5/5], Loss: 0.3776\n",
      "Epoch [83/100] complete, Total Loss: 1.3975\n",
      "Epoch [84/100], Batch [1/5], Loss: 0.2096\n",
      "Epoch [84/100], Batch [2/5], Loss: 0.3047\n",
      "Epoch [84/100], Batch [3/5], Loss: 0.2150\n",
      "Epoch [84/100], Batch [4/5], Loss: 0.3849\n",
      "Epoch [84/100], Batch [5/5], Loss: 0.2330\n",
      "Epoch [84/100] complete, Total Loss: 1.3472\n",
      "Epoch [85/100], Batch [1/5], Loss: 0.2811\n",
      "Epoch [85/100], Batch [2/5], Loss: 0.2565\n",
      "Epoch [85/100], Batch [3/5], Loss: 0.3388\n",
      "Epoch [85/100], Batch [4/5], Loss: 0.1894\n",
      "Epoch [85/100], Batch [5/5], Loss: 0.2881\n",
      "Epoch [85/100] complete, Total Loss: 1.3539\n",
      "Epoch [86/100], Batch [1/5], Loss: 0.2771\n",
      "Epoch [86/100], Batch [2/5], Loss: 0.2655\n",
      "Epoch [86/100], Batch [3/5], Loss: 0.2353\n",
      "Epoch [86/100], Batch [4/5], Loss: 0.2217\n",
      "Epoch [86/100], Batch [5/5], Loss: 0.3524\n",
      "Epoch [86/100] complete, Total Loss: 1.3518\n",
      "Epoch [87/100], Batch [1/5], Loss: 0.2901\n",
      "Epoch [87/100], Batch [2/5], Loss: 0.3036\n",
      "Epoch [87/100], Batch [3/5], Loss: 0.2422\n",
      "Epoch [87/100], Batch [4/5], Loss: 0.3043\n",
      "Epoch [87/100], Batch [5/5], Loss: 0.1458\n",
      "Epoch [87/100] complete, Total Loss: 1.2861\n",
      "Epoch [88/100], Batch [1/5], Loss: 0.2036\n",
      "Epoch [88/100], Batch [2/5], Loss: 0.3499\n",
      "Epoch [88/100], Batch [3/5], Loss: 0.2057\n",
      "Epoch [88/100], Batch [4/5], Loss: 0.3177\n",
      "Epoch [88/100], Batch [5/5], Loss: 0.2051\n",
      "Epoch [88/100] complete, Total Loss: 1.2820\n",
      "Epoch [89/100], Batch [1/5], Loss: 0.3298\n",
      "Epoch [89/100], Batch [2/5], Loss: 0.2889\n",
      "Epoch [89/100], Batch [3/5], Loss: 0.2744\n",
      "Epoch [89/100], Batch [4/5], Loss: 0.1384\n",
      "Epoch [89/100], Batch [5/5], Loss: 0.2478\n",
      "Epoch [89/100] complete, Total Loss: 1.2794\n",
      "Epoch [90/100], Batch [1/5], Loss: 0.2076\n",
      "Epoch [90/100], Batch [2/5], Loss: 0.0978\n",
      "Epoch [90/100], Batch [3/5], Loss: 0.2410\n",
      "Epoch [90/100], Batch [4/5], Loss: 0.4044\n",
      "Epoch [90/100], Batch [5/5], Loss: 0.3499\n",
      "Epoch [90/100] complete, Total Loss: 1.3006\n",
      "Epoch [91/100], Batch [1/5], Loss: 0.2290\n",
      "Epoch [91/100], Batch [2/5], Loss: 0.2509\n",
      "Epoch [91/100], Batch [3/5], Loss: 0.3722\n",
      "Epoch [91/100], Batch [4/5], Loss: 0.2561\n",
      "Epoch [91/100], Batch [5/5], Loss: 0.1130\n",
      "Epoch [91/100] complete, Total Loss: 1.2212\n",
      "Epoch [92/100], Batch [1/5], Loss: 0.2824\n",
      "Epoch [92/100], Batch [2/5], Loss: 0.1594\n",
      "Epoch [92/100], Batch [3/5], Loss: 0.3170\n",
      "Epoch [92/100], Batch [4/5], Loss: 0.2234\n",
      "Epoch [92/100], Batch [5/5], Loss: 0.2579\n",
      "Epoch [92/100] complete, Total Loss: 1.2399\n",
      "Epoch [93/100], Batch [1/5], Loss: 0.3913\n",
      "Epoch [93/100], Batch [2/5], Loss: 0.1777\n",
      "Epoch [93/100], Batch [3/5], Loss: 0.2335\n",
      "Epoch [93/100], Batch [4/5], Loss: 0.2020\n",
      "Epoch [93/100], Batch [5/5], Loss: 0.2098\n",
      "Epoch [93/100] complete, Total Loss: 1.2143\n",
      "Epoch [94/100], Batch [1/5], Loss: 0.3866\n",
      "Epoch [94/100], Batch [2/5], Loss: 0.1654\n",
      "Epoch [94/100], Batch [3/5], Loss: 0.2073\n",
      "Epoch [94/100], Batch [4/5], Loss: 0.2259\n",
      "Epoch [94/100], Batch [5/5], Loss: 0.2159\n",
      "Epoch [94/100] complete, Total Loss: 1.2011\n",
      "Epoch [95/100], Batch [1/5], Loss: 0.2730\n",
      "Epoch [95/100], Batch [2/5], Loss: 0.1771\n",
      "Epoch [95/100], Batch [3/5], Loss: 0.2073\n",
      "Epoch [95/100], Batch [4/5], Loss: 0.3176\n",
      "Epoch [95/100], Batch [5/5], Loss: 0.2109\n",
      "Epoch [95/100] complete, Total Loss: 1.1859\n",
      "Epoch [96/100], Batch [1/5], Loss: 0.3695\n",
      "Epoch [96/100], Batch [2/5], Loss: 0.1930\n",
      "Epoch [96/100], Batch [3/5], Loss: 0.2160\n",
      "Epoch [96/100], Batch [4/5], Loss: 0.2085\n",
      "Epoch [96/100], Batch [5/5], Loss: 0.1779\n",
      "Epoch [96/100] complete, Total Loss: 1.1649\n",
      "Epoch [97/100], Batch [1/5], Loss: 0.1739\n",
      "Epoch [97/100], Batch [2/5], Loss: 0.2711\n",
      "Epoch [97/100], Batch [3/5], Loss: 0.3295\n",
      "Epoch [97/100], Batch [4/5], Loss: 0.1484\n",
      "Epoch [97/100], Batch [5/5], Loss: 0.2458\n",
      "Epoch [97/100] complete, Total Loss: 1.1688\n",
      "Epoch [98/100], Batch [1/5], Loss: 0.1355\n",
      "Epoch [98/100], Batch [2/5], Loss: 0.2595\n",
      "Epoch [98/100], Batch [3/5], Loss: 0.2531\n",
      "Epoch [98/100], Batch [4/5], Loss: 0.2851\n",
      "Epoch [98/100], Batch [5/5], Loss: 0.2129\n",
      "Epoch [98/100] complete, Total Loss: 1.1461\n",
      "Epoch [99/100], Batch [1/5], Loss: 0.1771\n",
      "Epoch [99/100], Batch [2/5], Loss: 0.3462\n",
      "Epoch [99/100], Batch [3/5], Loss: 0.2199\n",
      "Epoch [99/100], Batch [4/5], Loss: 0.2329\n",
      "Epoch [99/100], Batch [5/5], Loss: 0.1422\n",
      "Epoch [99/100] complete, Total Loss: 1.1184\n",
      "Epoch [100/100], Batch [1/5], Loss: 0.2611\n",
      "Epoch [100/100], Batch [2/5], Loss: 0.1839\n",
      "Epoch [100/100], Batch [3/5], Loss: 0.2141\n",
      "Epoch [100/100], Batch [4/5], Loss: 0.2183\n",
      "Epoch [100/100], Batch [5/5], Loss: 0.2484\n",
      "Epoch [100/100] complete, Total Loss: 1.1257\n",
      "CNN training complete.\n",
      "Model architecture saved to seismic_cnn_model_architecture.json\n",
      "Model weights saved to seismic_cnn_model_weights.pth\n",
      "Full model saved to seismic_cnn_model_full.pth\n",
      "CNN model and weights saved.\n"
     ]
    }
   ],
   "source": [
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the Code and Results\n",
    "1. Data Preprocessing\n",
    "\n",
    "The code handles seismic data preprocessing using time series data in either .mseed or .csv format. The preprocessing steps are as follows:\n",
    "\n",
    "    Bandpass Filtering: The seismic signal is filtered to retain frequencies between 0.5 Hz and 1.0 Hz to remove noise outside this range.\n",
    "    STA/LTA Feature Extraction: The code computes the Short-Term Average/Long-Term Average (STA/LTA) ratio for each time series. This feature extraction technique helps detect anomalies or abrupt changes in seismic data, such as earthquakes.\n",
    "\n",
    "The STA/LTA ratios are then truncated or padded to a fixed length of 500 samples for each signal, ensuring a uniform input size for the CNN model.\n",
    "2. Data Loading and Labeling\n",
    "\n",
    "The load_seismic_data function reads seismic data from either .mseed or .csv files and matches them with their corresponding event labels from a catalog dataframe (catalog_df). The labels (mq_type) are converted to numeric values using LabelEncoder, which maps event types (likely impact, deep, and shallow seismic events) to integers [0, 1, 2].\n",
    "3. CNN Model Architecture\n",
    "\n",
    "The CNN model, defined as SeismicCNN, is a basic two-layer 1D convolutional neural network designed to classify seismic events into three categories:\n",
    "\n",
    "    Layer 1: A 1D convolutional layer with 64 filters followed by max pooling.\n",
    "    Layer 2: A second 1D convolutional layer with 128 filters, also followed by max pooling.\n",
    "    Fully Connected Layers: Two fully connected (dense) layers are dynamically initialized based on the flattened output size after the convolutional layers. The final output has 3 units, corresponding to the 3 seismic event categories.\n",
    "\n",
    "The model uses ReLU activation functions and CrossEntropyLoss as the loss function, appropriate for multi-class classification problems.\n",
    "4. Model Training and Performance\n",
    "\n",
    "The model is trained for 100 epochs, with 5 batches per epoch, on a dataset of 152 samples with 500 features each. The training process outputs the loss after each batch and computes the total loss at the end of each epoch. Here is an analysis of the training results:\n",
    "\n",
    "    Initial Loss: The loss starts at a relatively high value (4.6908 at Epoch 1) and gradually decreases as the model learns, indicating that the model is improving its predictions over time.\n",
    "    Convergence: The total loss consistently decreases with each epoch, reaching a final loss of 1.1257 after 100 epochs. This suggests that the model is learning and converging as expected.\n",
    "    Learning Rate Stability: The use of the Adam optimizer (with a learning rate of 0.001) helps maintain a steady improvement in loss reduction without sharp fluctuations, showing that the learning process is stable.\n",
    "\n",
    "The training loss decreases steadily, which is a good sign. However, without a validation set, it is hard to say whether the model is overfitting to the training data or if it generalizes well to new data. Introducing a validation set could help track the model's performance on unseen data and ensure it isn't overfitting.\n",
    "5. Model Saving\n",
    "\n",
    "The model architecture and weights are saved using three different methods:\n",
    "\n",
    "    Model architecture in JSON format: The architecture details (convolutional layers, filter sizes, etc.) are saved in a JSON file.\n",
    "    Model weights in a .pth file: The trained model's weights are saved in PyTorchâ€™s binary format.\n",
    "    Full model: Both the architecture and weights are saved together for later use.\n",
    "\n",
    "This modular saving approach ensures that you can reload the model and its weights for further testing or deployment.\n",
    "6. Key Observations from the Training Process\n",
    "\n",
    "    Initial Learning: The first few epochs show rapid improvements in the loss (Epoch 1 total loss is 4.6908, and Epoch 2 drops to 3.8972). This is typical in early-stage training, where the model quickly adjusts its weights to reduce the error.\n",
    "    Gradual Improvement: After initial epochs, the improvements become more gradual, which is expected as the model fine-tunes the weights and learns more subtle patterns in the data.\n",
    "    Epoch 100 Results: By the end of Epoch 100, the model has reduced the total loss to 1.1257, which suggests the model is well-trained on the dataset.\n",
    "\n",
    "7. Limitations and Suggestions\n",
    "\n",
    "    Absence of a Validation Set: There is no validation set in the training process to check for overfitting. Adding a validation dataset and evaluating metrics like accuracy, precision, recall, and F1-score would help assess model performance more thoroughly.\n",
    "    Class Imbalance Check: The unique labels are [0, 1, 2], which correspond to the different seismic event types. While the labels appear balanced, verifying the distribution of these labels would ensure no class imbalance, which could bias the model towards the majority class.\n",
    "    Potential Overfitting: Since the dataset is small (152 samples), the model may overfit. Techniques like regularization (e.g., dropout) or data augmentation could help mitigate overfitting risks.\n",
    "    Testing on Unlabeled Data: Once the model is trained, it can be tested on unlabeled .mseed files to see if it can accurately detect seismic anomalies in new data.\n",
    "\n",
    "8. Final Recommendations\n",
    "\n",
    "    Add a validation set: Split the dataset into training and validation subsets to better understand the model's generalization ability.\n",
    "    Evaluate on test data: After training, evaluate the model on new, unseen data to verify its real-world applicability.\n",
    "    Monitor evaluation metrics: Instead of relying only on the loss, track metrics like accuracy, precision, recall, and F1-score to understand how well the model performs on each class.\n",
    "    Test for generalization: Apply the model to unlabeled seismic data to check how well it can classify new events and potentially identify seismic anomalies in real-world scenarios.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The model shows strong learning capacity with a steady reduction in loss over 100 epochs. The architecture and training approach are sound, and the saved model can be used for further testing. However, to ensure the model generalizes well to unseen data, the inclusion of a validation set and comprehensive performance metrics (accuracy, precision, recall) is highly recommended."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
