{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from obspy import read\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply padding and create a mask\n",
    "def pad_sequences(sequences, max_len=None, padding_value=0):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "    padded_seqs = np.full((len(sequences), max_len), padding_value, dtype=np.float32)\n",
    "    masks = np.zeros((len(sequences), max_len), dtype=np.float32)\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = len(seq)\n",
    "        padded_seqs[i, :seq_len] = seq\n",
    "        masks[i, :seq_len] = 1  # Valid data points\n",
    "\n",
    "    return padded_seqs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for both .csv and .mseed\n",
    "def preprocess_seismic_data(filepath, filetype, sampling_rate=6.625):\n",
    "    if filetype == 'csv':\n",
    "        seismic_data = pd.read_csv(filepath)\n",
    "        trace = seismic_data['velocity(m/s)'].values\n",
    "    elif filetype == 'mseed':\n",
    "        st = read(filepath)\n",
    "        trace = st[0].data\n",
    "    \n",
    "    sos = signal.butter(4, [0.5, 1.0], btype='bandpass', fs=sampling_rate, output='sos')\n",
    "    filtered_trace = signal.sosfilt(sos, trace)\n",
    "\n",
    "    return filtered_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the catalog for training\n",
    "def load_catalog(catalog_file):\n",
    "    catalog_df = pd.read_csv(catalog_file)\n",
    "    \n",
    "    # One-hot encoding of the 'mq_type' column\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    one_hot_labels = encoder.fit_transform(catalog_df[['mq_type']])\n",
    "    \n",
    "    # Add the one-hot encoded columns to the catalog\n",
    "    one_hot_label_columns = encoder.categories_[0]\n",
    "    catalog_df = catalog_df.join(pd.DataFrame(one_hot_labels, columns=one_hot_label_columns))\n",
    "    \n",
    "    return catalog_df, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data files (csv or mseed) from the directory\n",
    "def load_seismic_data(data_dir, catalog_df=None, include_catalog=False):\n",
    "    seismic_data = []\n",
    "    labels = []\n",
    "    \n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            if file.endswith('.csv'):\n",
    "                filetype = 'csv'\n",
    "            elif file.endswith('.mseed'):\n",
    "                filetype = 'mseed'\n",
    "            else:\n",
    "                continue  # Skip unsupported file types\n",
    "            \n",
    "            # Preprocess seismic data\n",
    "            filtered_trace = preprocess_seismic_data(filepath, filetype)\n",
    "            seismic_data.append(filtered_trace)\n",
    "            \n",
    "            if include_catalog and catalog_df is not None:\n",
    "                # Extract label from catalog based on file name (if it exists)\n",
    "                event_id = os.path.splitext(file)[0]\n",
    "                if event_id in catalog_df['filename'].values:\n",
    "                    label_row = catalog_df.loc[catalog_df['filename'] == event_id]\n",
    "                    labels.append(label_row.iloc[0, -len(catalog_df.columns[-3:]):].values)  # One-hot encoded labels\n",
    "    \n",
    "    # Padding and masking\n",
    "    seismic_data_padded, masks = pad_sequences(seismic_data)\n",
    "    \n",
    "    seismic_data_padded = np.array(seismic_data_padded)\n",
    "    masks = np.array(masks)\n",
    "\n",
    "    if include_catalog:\n",
    "        return seismic_data_padded, labels, masks\n",
    "    else:\n",
    "        return seismic_data_padded, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract STA/LTA features with fixed length\n",
    "def extract_sta_lta_features(tr_data, tr_times, sta_len=120, lta_len=600, fixed_length=500):\n",
    "    df = len(tr_times) / (tr_times[-1] - tr_times[0])  # Estimate sampling rate\n",
    "    cft = classic_sta_lta(tr_data, int(sta_len * df), int(lta_len * df))\n",
    "\n",
    "    if len(cft) > fixed_length:\n",
    "        features = cft[:fixed_length]  # Truncate if longer\n",
    "    else:\n",
    "        # Pad with zeros if shorter\n",
    "        features = np.pad(cft, (0, fixed_length - len(cft)), 'constant')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data from traces and extract features\n",
    "def prepare_data_from_traces(catalog, data_directory, use_mseed=True):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for idx, row in catalog.iterrows():\n",
    "        test_filename = row['filename']\n",
    "        arrival_time = row['time_rel(sec)']\n",
    "\n",
    "        if use_mseed:\n",
    "            mseed_file = os.path.join(data_directory, f\"S12_GradeA/{test_filename}.mseed\")\n",
    "            if not os.path.exists(mseed_file):\n",
    "                continue\n",
    "            st = read(mseed_file)\n",
    "            tr = st.traces[0]\n",
    "            tr_data = tr.data\n",
    "            tr_times = tr.times()\n",
    "        else:\n",
    "            csv_file = os.path.join(data_directory, f\"S12_GradeA/{test_filename}.csv\")\n",
    "            if not os.path.exists(csv_file):\n",
    "                continue\n",
    "            data_cat = pd.read_csv(csv_file)\n",
    "            tr_data = np.array(data_cat['velocity(m/s)'])\n",
    "            tr_times = np.array(data_cat['time_rel(sec)'])\n",
    "\n",
    "        # Extract features using STA/LTA with a fixed length\n",
    "        features = extract_sta_lta_features(tr_data, tr_times)\n",
    "        features_list.append(features)\n",
    "        labels_list.append(1)  # All catalog events are considered 1 for simplicity\n",
    "\n",
    "    X = np.array(features_list)\n",
    "    y = np.array(labels_list)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (100, 50)\n",
      "Shape of y: (100,)\n",
      "y was flattened to: (100,)\n",
      "Processed data saved to processed_data_catalog.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to save processed data into a catalog with added debugging\n",
    "def save_processed_data(X, y, save_path='processed_data_catalog.csv'):\n",
    "    \"\"\"\n",
    "    Save the processed features (X) and labels (y) into a CSV file for future use.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy array): The features data, expected to be a 2D array or reshaped as needed.\n",
    "    y (numpy array): The labels.\n",
    "    save_path (str): The file path where the data will be saved.\n",
    "    \"\"\"\n",
    "    # Check shapes of X and y for debugging\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape}\")\n",
    "    \n",
    "    # Reshape X if necessary to ensure it's 2D (samples, features)\n",
    "    if len(X.shape) > 2:\n",
    "        # Flatten each trace if it's multi-dimensional (e.g., 3D or more)\n",
    "        X_flattened = X.reshape(X.shape[0], -1)\n",
    "        print(f\"X was reshaped to: {X_flattened.shape}\")\n",
    "    else:\n",
    "        X_flattened = X\n",
    "\n",
    "    # Ensure y is flattened for single-label output\n",
    "    y_flattened = y.flatten()\n",
    "    print(f\"y was flattened to: {y_flattened.shape}\")\n",
    "\n",
    "    # Convert features and labels into a DataFrame for easy storage\n",
    "    try:\n",
    "        data = pd.DataFrame(X_flattened)\n",
    "        data['label'] = y_flattened\n",
    "\n",
    "        # Save to CSV\n",
    "        data.to_csv(save_path, index=False)\n",
    "        print(f\"Processed data saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while saving data: {str(e)}\")\n",
    "\n",
    "# Example usage (mock data for demonstration purposes)\n",
    "if __name__ == \"__main__\":\n",
    "    # Example mock data to test saving functionality\n",
    "    X_example = np.random.rand(100, 50)  # 100 samples, 50 features\n",
    "    y_example = np.random.randint(0, 2, 100)  # 100 labels (binary classification)\n",
    "    \n",
    "    save_processed_data(X_example, y_example, 'processed_data_catalog.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load processed data from a saved catalog\n",
    "def load_processed_data(load_path='processed_data_catalog.csv'):\n",
    "    \"\"\"\n",
    "    Load the processed features and labels from a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    load_path (str): The file path from which the data will be loaded.\n",
    "    \n",
    "    Returns:\n",
    "    X (numpy array): The loaded features.\n",
    "    y (numpy array): The loaded labels.\n",
    "    \"\"\"\n",
    "    # Load data from CSV\n",
    "    data = pd.read_csv(load_path)\n",
    "    \n",
    "    # Separate features and labels\n",
    "    X = data.iloc[:, :-1].values  # All columns except the last\n",
    "    y = data['label'].values  # The last column is the label\n",
    "\n",
    "    print(f\"Processed data loaded from {load_path}\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Decision Tree\n",
    "def train_decision_tree(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a Decision Tree Classifier\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "    dt_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = dt_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Classification Report for Decision Tree:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return dt_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from catalog...\n",
      "Processed data loaded from processed_data_catalog.csv\n",
      "Classification Report for Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.40      0.44        10\n",
      "           1       0.50      0.60      0.55        10\n",
      "\n",
      "    accuracy                           0.50        20\n",
      "   macro avg       0.50      0.50      0.49        20\n",
      "weighted avg       0.50      0.50      0.49        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modified main function to include saving and loading of processed data\n",
    "def main():\n",
    "    catalog_path = '../../data/lunar_data/training/catalogs/apollo12_catalog_GradeA_final.csv'\n",
    "    data_directory = '../../data/lunar_data/training/data/'\n",
    "    processed_data_path = 'processed_data_catalog.csv'\n",
    "\n",
    "    # Check if preprocessed data exists\n",
    "    if os.path.exists(processed_data_path):\n",
    "        print(\"Loading processed data from catalog...\")\n",
    "        X, y = load_processed_data(processed_data_path)\n",
    "    else:\n",
    "        # If no processed data is found, preprocess and save\n",
    "        print(\"Preprocessing data...\")\n",
    "        catalog = pd.read_csv(catalog_path)\n",
    "        X, y = prepare_data_from_traces(catalog, data_directory)\n",
    "\n",
    "        # Save the processed data for future use\n",
    "        save_processed_data(X, y, processed_data_path)\n",
    "\n",
    "    # Train and evaluate Decision Tree\n",
    "    dt_model = train_decision_tree(X, y)\n",
    "\n",
    "# %%\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
