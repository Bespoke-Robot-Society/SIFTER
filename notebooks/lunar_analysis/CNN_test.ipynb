{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy import read\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import timedelta\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, ConfusionMatrixDisplay\n",
    "\n",
    "# Set device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Directory paths\n",
    "lunar_catalog_path = '../../data/lunar_data/training/catalogs/apollo12_catalog_GradeA_final.csv'\n",
    "lunar_data_directory = '../../data/lunar_data/training/data/S12_GradeA/'\n",
    "martian_data_directory = '../../data/marsquake_data/training/data/'\n",
    "lunar_data_images_dir = '../../model/model_output/lunar_preprocessed_images/'\n",
    "martian_data_images_dir = '../../model/model_output/martian_preprocessed_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def convert_rel_to_abs_time(start_time, time_rel):\n",
    "    return (start_time + timedelta(seconds=float(time_rel))).strftime('%Y-%m-%dT%H:%M:%S.%f')\n",
    "\n",
    "def apply_bandpass_filter(trace, sampling_rate, freqmin=0.5, freqmax=3.0):\n",
    "    sos = signal.butter(4, [freqmin, freqmax], btype='bandpass', fs=sampling_rate, output='sos')\n",
    "    return signal.sosfilt(sos, trace)\n",
    "\n",
    "def load_existing_images(image_dir):\n",
    "    image_files = [os.path.join(root, file) for root, _, files in os.walk(image_dir) for file in files if file.endswith('.png')]\n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Functions\n",
    "def preprocess_lunar_data(lunar_data_directory):\n",
    "    lunar_images = load_existing_images(lunar_data_directory)\n",
    "    print(f\"{len(lunar_images)} lunar images loaded.\")\n",
    "    return lunar_images\n",
    "\n",
    "def preprocess_martian_data(data_dir, save_dir, combine_images=True):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    martian_images, martian_arrival_times = [], []\n",
    "    mseed_files = [f for f in os.listdir(data_dir) if f.endswith('.mseed')]\n",
    "\n",
    "    if len(mseed_files) == 0:\n",
    "        print(\"No .mseed files found in the directory.\")\n",
    "        return martian_images, martian_arrival_times\n",
    "\n",
    "    for filename in mseed_files:\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        csv_file_path = file_path.replace('.mseed', '.csv')\n",
    "\n",
    "        if not os.path.exists(csv_file_path):\n",
    "            print(f\"CSV file not found for {filename}: {csv_file_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            csv_data = pd.read_csv(csv_file_path)\n",
    "            arrival_time_rel = csv_data['rel_time(sec)'].iloc[0]\n",
    "            image_path = plot_and_save_trace_spectrogram(file_path, arrival_time_rel, save_dir, filename, combine_images)\n",
    "            martian_images.append(image_path)\n",
    "            martian_arrival_times.append(arrival_time_rel)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    return martian_images, martian_arrival_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model Definition\n",
    "class SpectrogramCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpectrogramCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 112 * 112, 128) #edit to None if it doesn't work\n",
    "        self.fc_event = nn.Linear(128, 3)\n",
    "        self.fc_time = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(x.size(1), 128)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc_event(x), self.fc_time(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "def prepare_data_for_training(image_files, labels, time_labels, batch_size=32):\n",
    "    if not image_files:\n",
    "        return None  # Early exit if no images are provided\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    image_tensors = [transform(Image.open(img)) for img in image_files if os.path.exists(img) and img.endswith('.png')]\n",
    "    if image_tensors:\n",
    "        X_tensor = torch.stack(image_tensors)\n",
    "        y_event_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "        y_time_tensor = torch.tensor(time_labels, dtype=torch.float32)\n",
    "        dataset = TensorDataset(X_tensor, y_event_tensor, y_time_tensor)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return None\n",
    "\n",
    "# Training Functions\n",
    "def train_model(model, train_loader, criterion_event, criterion_time, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, event_labels, time_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            event_output, time_output = model(inputs)\n",
    "            loss_event = criterion_event(event_output, event_labels)\n",
    "            loss_time = criterion_time(time_output.squeeze(), time_labels)\n",
    "            loss = loss_event + loss_time\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "def self_train_on_martian_data(model, martian_data_loader, optimizer, criterion_event, criterion_time, num_epochs=10):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in martian_data_loader:\n",
    "            images = batch[0]  # Unpack the images from the batch tuple\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the model\n",
    "            event_output, time_output = model(images)\n",
    "\n",
    "            # Generate pseudo-labels (predicted event labels)\n",
    "            _, pseudo_labels = torch.max(event_output, 1)\n",
    "\n",
    "            # For now, we do not have ground truth for Martian data, so we only optimize on pseudo-labels\n",
    "            loss_event = criterion_event(event_output, pseudo_labels)\n",
    "            loss_time = criterion_time(time_output.squeeze(), torch.zeros_like(time_output.squeeze()))  # Use zeros as placeholder\n",
    "\n",
    "            # Calculate total loss and perform backpropagation\n",
    "            total_loss = loss_event + loss_time\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "        print(f\"Self-training Epoch {epoch+1}, Loss: {running_loss/len(martian_data_loader)}\")\n",
    "\n",
    "# Model Evaluation\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    event_preds, time_preds, event_true, time_true = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for images, event_labels, time_labels in data_loader:\n",
    "            event_output, time_output = model(images)\n",
    "            _, event_pred_classes = torch.max(event_output, 1)\n",
    "            event_preds.extend(event_pred_classes.cpu().numpy())\n",
    "            time_preds.extend(time_output.cpu().numpy())\n",
    "            event_true.extend(event_labels.cpu().numpy())\n",
    "            time_true.extend(time_labels.cpu().numpy())\n",
    "    event_accuracy = accuracy_score(event_true, event_preds)\n",
    "    time_mae = mean_absolute_error(time_true, time_preds)\n",
    "    print(f\"Validation Event Accuracy: {event_accuracy:.4f}\")\n",
    "    print(f\"Validation Time MAE: {time_mae:.4f}\")\n",
    "    ConfusionMatrixDisplay.from_predictions(event_true, event_preds)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Save Function\n",
    "def save_model_artifacts(model, model_name='seismic_cnn_model'):\n",
    "    model_architecture = {\n",
    "        'conv_layers': [{'in_channels': model.conv1.in_channels, 'out_channels': model.conv1.out_channels, 'kernel_size': model.conv1.kernel_size},\n",
    "                        {'in_channels': model.conv2.in_channels, 'out_channels': model.conv2.out_channels, 'kernel_size': model.conv2.kernel_size}],\n",
    "        'fc_layers': [{'in_features': model.fc_event.in_features, 'out_features': model.fc_event.out_features},\n",
    "                      {'in_features': model.fc_time.in_features, 'out_features': model.fc_time.out_features}]\n",
    "    }\n",
    "    with open(f'{model_name}_architecture.json', 'w') as f:\n",
    "        json.dump(model_architecture, f, indent=4)\n",
    "    torch.save(model.state_dict(), f'{model_name}_weights.pth')\n",
    "    torch.save(model, f'{model_name}_full.pth')\n",
    "    print(f\"Model saved to {model_name}_full.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_image_list(image_list):\n",
    "    \"\"\"\n",
    "    Ensure image list is flat in case there are nested lists of image paths.\n",
    "    \"\"\"\n",
    "    if isinstance(image_list, (list, tuple)) and any(isinstance(i, (list, tuple)) for i in image_list):\n",
    "        return [item for sublist in image_list for item in sublist]\n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_unlabeled_data_loader(image_files, batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    image_tensors = [transform(Image.open(img)) for img in image_files if os.path.exists(img) and img.endswith('.png')]\n",
    "    if image_tensors:\n",
    "        X_tensor = torch.stack(image_tensors)\n",
    "        dataset = TensorDataset(X_tensor)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define the paths for preprocessed images\n",
    "    lunar_data_images_dir = '../../model/model_output/lunar_preprocessed_images/'\n",
    "    martian_data_images_dir = '../../model/model_output/martian_preprocessed_images/'\n",
    "    save_dir = martian_data_images_dir  # Directory where Martian images are saved\n",
    "\n",
    "    # 1. Load pre-generated lunar images\n",
    "    print(f\"Loading pre-generated lunar data from: {lunar_data_images_dir}\")\n",
    "    lunar_images = preprocess_lunar_data(lunar_data_images_dir)\n",
    "\n",
    "    # Check if any lunar images were loaded\n",
    "    if not lunar_images:\n",
    "        print(\"Error: No lunar images found. Exiting.\")\n",
    "        return  # Exit early if no images found\n",
    "\n",
    "    lunar_labels = [0] * len(lunar_images)  # Placeholder labels for lunar data\n",
    "    lunar_times = [0] * len(lunar_images)   # Placeholder arrival times for lunar data\n",
    "\n",
    "    # Prepare DataLoader for lunar data\n",
    "    print(\"Preparing DataLoader for lunar data...\")\n",
    "    train_loader = prepare_data_for_training(lunar_images, lunar_labels, lunar_times)\n",
    "\n",
    "    if train_loader is None:\n",
    "        print(\"Error: No valid data for training. DataLoader creation failed.\")\n",
    "        return  # Exit early if DataLoader creation failed\n",
    "\n",
    "    # 2. Initialize and train the model on lunar data\n",
    "    print(\"Initializing SpectrogramCNN model...\")\n",
    "    model = SpectrogramCNN()\n",
    "    criterion_event = nn.CrossEntropyLoss()\n",
    "    criterion_time = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print(\"Training model on lunar data...\")\n",
    "    train_model(model, train_loader, criterion_event, criterion_time, optimizer)\n",
    "\n",
    "    # 3. Load pre-generated Martian images\n",
    "    print(f\"Loading pre-generated Martian data from: {martian_data_images_dir}\")\n",
    "    martian_images = load_existing_images(martian_data_images_dir)  # Load images directly\n",
    "\n",
    "    # Check if any Martian images were loaded\n",
    "    if not martian_images:\n",
    "        print(\"Error: No Martian images found. Exiting.\")\n",
    "        return  # Exit early if no images found\n",
    "\n",
    "    # Prepare DataLoader for Martian data\n",
    "    print(\"Preparing DataLoader for Martian data...\")\n",
    "    martian_data_loader = prepare_unlabeled_data_loader(martian_images)\n",
    "\n",
    "    if martian_data_loader is None:\n",
    "        print(\"Error: No valid data for self-training. DataLoader creation failed.\")\n",
    "        return  # Exit early if DataLoader creation failed\n",
    "\n",
    "    # 4. Self-train the model on Martian data\n",
    "    print(\"Self-training model on Martian data...\")\n",
    "    self_train_on_martian_data(model, martian_data_loader, optimizer, criterion_event, criterion_time)\n",
    "\n",
    "    # 5. Save the fine-tuned model\n",
    "    print(\"Saving the fine-tuned model...\")\n",
    "    save_model_artifacts(model, model_name='../../model/fine_tuned_martian_seismic_cnn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-generated lunar data from: ../../model/model_output/lunar_preprocessed_images/\n",
      "76 lunar images loaded.\n",
      "Preparing DataLoader for lunar data...\n",
      "Initializing SpectrogramCNN model...\n",
      "Training model on lunar data...\n",
      "Epoch 1, Loss: 1600.0661470890045\n",
      "Epoch 2, Loss: 449.5803413391113\n",
      "Epoch 3, Loss: 195.09067153930664\n",
      "Epoch 4, Loss: 3.13076926022768\n",
      "Epoch 5, Loss: 0.5083693067232767\n",
      "Epoch 6, Loss: 0.670348584651947\n",
      "Epoch 7, Loss: 0.8254997730255127\n",
      "Epoch 8, Loss: 0.9365084966023763\n",
      "Epoch 9, Loss: 0.9263431032498678\n",
      "Epoch 10, Loss: 0.9064249992370605\n",
      "Loading pre-generated Martian data from: ../../model/model_output/martian_preprocessed_images/\n",
      "Preparing DataLoader for Martian data...\n",
      "Self-training model on Martian data...\n",
      "Self-training Epoch 1, Loss: 0.8976970314979553\n",
      "Self-training Epoch 2, Loss: 0.889222264289856\n",
      "Self-training Epoch 3, Loss: 0.8802964687347412\n",
      "Self-training Epoch 4, Loss: 0.8709366917610168\n",
      "Self-training Epoch 5, Loss: 0.8611681461334229\n",
      "Self-training Epoch 6, Loss: 0.8509337902069092\n",
      "Self-training Epoch 7, Loss: 0.840172529220581\n",
      "Self-training Epoch 8, Loss: 0.828025221824646\n",
      "Self-training Epoch 9, Loss: 0.8153128623962402\n",
      "Self-training Epoch 10, Loss: 0.8020918369293213\n",
      "Saving the fine-tuned model...\n",
      "Model saved to ../../model/fine_tuned_martian_seismic_cnn_full.pth\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Analysis and Function Explanations\n",
    "Imports and Setup\n",
    "\n",
    "```python\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy import read\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import timedelta\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, ConfusionMatrixDisplay\n",
    "\n",
    "# Set device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Directory paths\n",
    "lunar_catalog_path = '../../data/lunar_data/training/catalogs/apollo12_catalog_GradeA_final.csv'\n",
    "lunar_data_directory = '../../data/lunar_data/training/data/S12_GradeA/'\n",
    "martian_data_directory = '../../data/marsquake_data/training/data/'\n",
    "lunar_data_images_dir = '../../model/model_output/lunar_preprocessed_images/'\n",
    "martian_data_images_dir = '../../model/model_output/martian_preprocessed_images/'\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    The code imports necessary libraries for data manipulation (numpy, pandas), signal processing (scipy, obspy), image processing (PIL, torchvision), machine learning (torch, sklearn), and visualization (matplotlib).\n",
    "    Sets the computation device to CPU.\n",
    "    Defines directory paths for lunar and Martian data and images.\n",
    "\n",
    "Utility Functions\n",
    "1. convert_rel_to_abs_time\n",
    "\n",
    "```python\n",
    "\n",
    "def convert_rel_to_abs_time(start_time, time_rel):\n",
    "    return (start_time + timedelta(seconds=float(time_rel))).strftime('%Y-%m-%dT%H:%M:%S.%f')\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    Purpose: Converts a relative time (in seconds) to an absolute timestamp by adding it to a start time.\n",
    "    Parameters:\n",
    "        start_time: The initial datetime object.\n",
    "        time_rel: Relative time in seconds.\n",
    "    Returns: A string representing the absolute time in a specific format.\n",
    "\n",
    "Optimization Suggestion:\n",
    "\n",
    "    Ensure that start_time is a datetime object. If start_time might be a string, parse it to datetime first.\n",
    "\n",
    "2. apply_bandpass_filter\n",
    "\n",
    "```python\n",
    "\n",
    "def apply_bandpass_filter(trace, sampling_rate, freqmin=0.5, freqmax=3.0):\n",
    "    sos = signal.butter(4, [freqmin, freqmax], btype='bandpass', fs=sampling_rate, output='sos')\n",
    "    return signal.sosfilt(sos, trace)\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    Purpose: Applies a Butterworth bandpass filter to a signal trace.\n",
    "    Parameters:\n",
    "        trace: The signal data as a 1D array.\n",
    "        sampling_rate: The sampling frequency of the signal.\n",
    "        freqmin: Minimum frequency for the bandpass filter.\n",
    "        freqmax: Maximum frequency for the bandpass filter.\n",
    "    Returns: Filtered signal.\n",
    "\n",
    "Optimization Suggestion:\n",
    "\n",
    "    Precompute the filter coefficients if the function is called multiple times with the same parameters to avoid redundant calculations.\n",
    "\n",
    "3. load_existing_images\n",
    "\n",
    "```python\n",
    "\n",
    "def load_existing_images(image_dir):\n",
    "    image_files = [os.path.join(root, file) for root, _, files in os.walk(image_dir) for file in files if file.endswith('.png')]\n",
    "    return image_files\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    Purpose: Recursively loads all .png image file paths from a directory.\n",
    "    Parameters:\n",
    "        image_dir: Directory containing images.\n",
    "    Returns: A list of image file paths.\n",
    "\n",
    "Optimization Suggestion:\n",
    "\n",
    "    Use glob.glob with recursive patterns for potentially faster execution.\n",
    "\n",
    "Preprocessing Functions\n",
    "4. preprocess_lunar_data\n",
    "\n",
    "```python\n",
    "\n",
    "def preprocess_lunar_data(lunar_data_directory):\n",
    "    lunar_images = load_existing_images(lunar_data_directory)\n",
    "    print(f\"{len(lunar_images)} lunar images loaded.\")\n",
    "    return lunar_images\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    Purpose: Loads preprocessed lunar images from a directory.\n",
    "    Parameters:\n",
    "        lunar_data_directory: Directory containing lunar images.\n",
    "    Returns: List of lunar image file paths.\n",
    "\n",
    "Optimization Suggestion:\n",
    "\n",
    "    Validate if images are correctly loaded and handle potential exceptions.\n",
    "\n",
    "5. preprocess_martian_data\n",
    "\n",
    "```python\n",
    "\n",
    "def preprocess_martian_data(data_dir, save_dir, combine_images=True):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    martian_images, martian_arrival_times = [], []\n",
    "    mseed_files = [f for f in os.listdir(data_dir) if f.endswith('.mseed')]\n",
    "\n",
    "    if len(mseed_files) == 0:\n",
    "        print(\"No .mseed files found in the directory.\")\n",
    "        return martian_images, martian_arrival_times\n",
    "\n",
    "    for filename in mseed_files:\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        csv_file_path = file_path.replace('.mseed', '.csv')\n",
    "\n",
    "        if not os.path.exists(csv_file_path):\n",
    "            print(f\"CSV file not found for {filename}: {csv_file_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            csv_data = pd.read_csv(csv_file_path)\n",
    "            arrival_time_rel = csv_data['rel_time(sec)'].iloc[0]\n",
    "            image_path = plot_and_save_trace_spectrogram(file_path, arrival_time_rel, save_dir, filename, combine_images)\n",
    "            martian_images.append(image_path)\n",
    "            martian_arrival_times.append(arrival_time_rel)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    return martian_images, martian_arrival_times\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    Purpose: Preprocesses Martian seismic data by generating spectrogram images and extracting arrival times.\n",
    "    Parameters:\n",
    "        data_dir: Directory containing Martian seismic data (.mseed files).\n",
    "        save_dir: Directory to save generated images.\n",
    "        combine_images: Flag to combine images (unused in code).\n",
    "    Returns:\n",
    "        martian_images: List of paths to generated images.\n",
    "        martian_arrival_times: List of relative arrival times.\n",
    "\n",
    "Optimization Suggestions:\n",
    "\n",
    "    The function plot_and_save_trace_spectrogram is not defined in the provided code. Ensure this function is implemented.\n",
    "    Use exception handling to catch specific exceptions.\n",
    "    Consider multithreading or multiprocessing for processing multiple files concurrently.\n",
    "\n",
    "CNN Model Definition\n",
    "6. SpectrogramCNN Class\n",
    "\n",
    "```python\n",
    "\n",
    "class SpectrogramCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpectrogramCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 112 * 112, 128)  # Edit to None if it doesn't work\n",
    "        self.fc_event = nn.Linear(128, 3)\n",
    "        self.fc_time = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(x.size(1), 128)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc_event(x), self.fc_time(x)\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    Purpose: Defines a Convolutional Neural Network (CNN) for processing spectrogram images.\n",
    "\n",
    "    Architecture:\n",
    "        Convolutional Layers:\n",
    "            conv1: Convolves input image with 32 filters of size 3x3.\n",
    "            conv2: Convolves output of conv1 with 64 filters of size 3x3.\n",
    "        Pooling Layer:\n",
    "            pool: Applies max pooling with a 2x2 kernel.\n",
    "        Fully Connected Layers:\n",
    "            fc1: Fully connected layer with an input size of 64 * 112 * 112 and output size of 128.\n",
    "            fc_event: Outputs event classification into 3 classes.\n",
    "            fc_time: Outputs a single value representing time.\n",
    "\n",
    "    Forward Pass:\n",
    "        Applies ReLU activation after each convolution.\n",
    "        Applies pooling after the second convolution.\n",
    "        Flattens the tensor for the fully connected layer.\n",
    "        Checks if fc1 is None (which shouldn't be the case here).\n",
    "        Applies ReLU after fc1.\n",
    "        Returns outputs from fc_event and fc_time.\n",
    "\n",
    "Optimization Suggestions:\n",
    "\n",
    "    Issue with fc1 Initialization:\n",
    "        The check if self.fc1 is None is redundant because fc1 is already defined.\n",
    "        However, if the input image size changes, 64 * 112 * 112 may not match the flattened size, causing a size mismatch error.\n",
    "        Solution: Instead of hardcoding the input size for fc1, compute it dynamically:\n",
    "\n",
    "        ```python\n",
    "\n",
    "        def __init__(self):\n",
    "            super(SpectrogramCNN, self).__init__()\n",
    "            # ... [same as before]\n",
    "            self.fc1 = None  # Initialize fc1 as None\n",
    "\n",
    "        def forward(self, x):\n",
    "            # ... [same as before]\n",
    "            x = x.view(x.size(0), -1)\n",
    "            if self.fc1 is None:\n",
    "                self.fc1 = nn.Linear(x.size(1), 128)\n",
    "            # ... [same as before]\n",
    "```\n",
    "        This ensures fc1 adapts to the input size dynamically.\n",
    "\n",
    "    Consider Using Pretrained Models:\n",
    "        For better performance, consider using a pretrained model like ResNet or VGG with adjusted input channels.\n",
    "\n",
    "Data Preparation\n",
    "7. prepare_data_for_training\n",
    "\n",
    "```python\n",
    "\n",
    "def prepare_data_for_training(image_files, labels, time_labels, batch_size=32):\n",
    "    if not image_files:\n",
    "        return None  # Early exit if no images are provided\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    image_tensors = [transform(Image.open(img)) for img in image_files if os.path.exists(img) and img.endswith('.png')]\n",
    "    if image_tensors:\n",
    "        X_tensor = torch.stack(image_tensors)\n",
    "        y_event_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "        y_time_tensor = torch.tensor(time_labels, dtype=torch.float32)\n",
    "        dataset = TensorDataset(X_tensor, y_event_tensor, y_time_tensor)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return None\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    Purpose: Prepares a DataLoader for training by loading images and labels.\n",
    "    Parameters:\n",
    "        image_files: List of image file paths.\n",
    "        labels: List of event labels.\n",
    "        time_labels: List of time labels.\n",
    "        batch_size: Batch size for DataLoader.\n",
    "    Process:\n",
    "        Applies transformations to images (grayscale, resize, tensor conversion, normalization).\n",
    "        Stacks image tensors.\n",
    "        Creates tensors for labels.\n",
    "        Constructs a TensorDataset and returns a DataLoader.\n",
    "\n",
    "Optimization Suggestions:\n",
    "\n",
    "    Handle exceptions when opening images.\n",
    "    Ensure labels and time_labels are aligned with image_tensors.\n",
    "    Use torchvision.datasets.ImageFolder if the images are organized in folders by class.\n",
    "\n",
    "Training Functions\n",
    "8. train_model\n",
    "\n",
    "```python\n",
    "\n",
    "def train_model(model, train_loader, criterion_event, criterion_time, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, event_labels, time_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            event_output, time_output = model(inputs)\n",
    "            loss_event = criterion_event(event_output, event_labels)\n",
    "            loss_time = criterion_time(time_output.squeeze(), time_labels)\n",
    "            loss = loss_event + loss_time\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    Purpose: Trains the CNN model on the provided training data.\n",
    "    Parameters:\n",
    "        model: The CNN model.\n",
    "        train_loader: DataLoader with training data.\n",
    "        criterion_event: Loss function for event classification.\n",
    "        criterion_time: Loss function for time regression.\n",
    "        optimizer: Optimizer for model parameters.\n",
    "        num_epochs: Number of training epochs.\n",
    "    Process:\n",
    "        Sets the model to training mode.\n",
    "        Iterates over epochs and batches, computes losses, backpropagates, and updates parameters.\n",
    "        Prints the average loss per epoch.\n",
    "\n",
    "Optimization Suggestions:\n",
    "\n",
    "    Add validation after each epoch to monitor overfitting.\n",
    "    Implement learning rate scheduling.\n",
    "    Consider logging with tools like TensorBoard.\n",
    "\n",
    "9. self_train_on_martian_data\n",
    "\n",
    "```python\n",
    "\n",
    "def self_train_on_martian_data(model, martian_data_loader, optimizer, criterion_event, criterion_time, num_epochs=10):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in martian_data_loader:\n",
    "            images = batch[0]  # Unpack the images from the batch tuple\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the model\n",
    "            event_output, time_output = model(images)\n",
    "\n",
    "            # Generate pseudo-labels (predicted event labels)\n",
    "            _, pseudo_labels = torch.max(event_output, 1)\n",
    "\n",
    "            # For now, we do not have ground truth for Martian data, so we only optimize on pseudo-labels\n",
    "            loss_event = criterion_event(event_output, pseudo_labels)\n",
    "            loss_time = criterion_time(time_output.squeeze(), torch.zeros_like(time_output.squeeze()))  # Use zeros as placeholder\n",
    "\n",
    "            # Calculate total loss and perform backpropagation\n",
    "            total_loss = loss_event + loss_time\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "        print(f\"Self-training Epoch {epoch+1}, Loss: {running_loss/len(martian_data_loader)}\")\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    Purpose: Performs self-training on Martian data using pseudo-labels generated by the model.\n",
    "    Parameters:\n",
    "        martian_data_loader: DataLoader with Martian images (without labels).\n",
    "    Process:\n",
    "        Sets the model to training mode.\n",
    "        For each batch, predicts labels and uses them as pseudo-labels.\n",
    "        Computes loss and updates model parameters.\n",
    "        Prints average loss per epoch.\n",
    "\n",
    "Optimization Suggestions:\n",
    "\n",
    "    Be cautious with self-training; incorrect pseudo-labels can degrade model performance.\n",
    "    Consider using confidence thresholds to select only high-confidence predictions for training.\n",
    "    Implement techniques like semi-supervised learning methods.\n",
    "\n",
    "Model Evaluation\n",
    "10. evaluate_model\n",
    "\n",
    "```python\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    event_preds, time_preds, event_true, time_true = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for images, event_labels, time_labels in data_loader:\n",
    "            event_output, time_output = model(images)\n",
    "            _, event_pred_classes = torch.max(event_output, 1)\n",
    "            event_preds.extend(event_pred_classes.cpu().numpy())\n",
    "            time_preds.extend(time_output.cpu().numpy())\n",
    "            event_true.extend(event_labels.cpu().numpy())\n",
    "            time_true.extend(time_labels.cpu().numpy())\n",
    "    event_accuracy = accuracy_score(event_true, event_preds)\n",
    "    time_mae = mean_absolute_error(time_true, time_preds)\n",
    "    print(f\"Validation Event Accuracy: {event_accuracy:.4f}\")\n",
    "    print(f\"Validation Time MAE: {time_mae:.4f}\")\n",
    "    ConfusionMatrixDisplay.from_predictions(event_true, event_preds)\n",
    "    plt.show()\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    Purpose: Evaluates the model's performance on a validation dataset.\n",
    "    Parameters:\n",
    "        data_loader: DataLoader with validation data.\n",
    "    Process:\n",
    "        Sets the model to evaluation mode.\n",
    "        Disables gradient computation.\n",
    "        Collects predictions and true labels.\n",
    "        Calculates accuracy and mean absolute error (MAE).\n",
    "        Displays a confusion matrix.\n",
    "\n",
    "Optimization Suggestions:\n",
    "\n",
    "    Use GPU if available to speed up evaluation.\n",
    "    Consider batch-wise evaluation for large datasets.\n",
    "\n",
    "Model Save Function\n",
    "11. save_model_artifacts\n",
    "\n",
    "```python\n",
    "\n",
    "def save_model_artifacts(model, model_name='seismic_cnn_model'):\n",
    "    model_architecture = {\n",
    "        'conv_layers': [{'in_channels': model.conv1.in_channels, 'out_channels': model.conv1.out_channels, 'kernel_size': model.conv1.kernel_size},\n",
    "                        {'in_channels': model.conv2.in_channels, 'out_channels': model.conv2.out_channels, 'kernel_size': model.conv2.kernel_size}],\n",
    "        'fc_layers': [{'in_features': model.fc_event.in_features, 'out_features': model.fc_event.out_features},\n",
    "                      {'in_features': model.fc_time.in_features, 'out_features': model.fc_time.out_features}]\n",
    "    }\n",
    "    with open(f'{model_name}_architecture.json', 'w') as f:\n",
    "        json.dump(model_architecture, f, indent=4)\n",
    "    torch.save(model.state_dict(), f'{model_name}_weights.pth')\n",
    "    torch.save(model, f'{model_name}_full.pth')\n",
    "    print(f\"Model saved to {model_name}_full.pth\")\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    Purpose: Saves the model's architecture and weights for future use.\n",
    "    Parameters:\n",
    "        model: The trained CNN model.\n",
    "        model_name: Base name for saved files.\n",
    "    Process:\n",
    "        Extracts model architecture details into a dictionary.\n",
    "        Saves the architecture as a JSON file.\n",
    "        Saves the model's state dictionary (weights) and the full model.\n",
    "\n",
    "Optimization Suggestions:\n",
    "\n",
    "    For version control, include training hyperparameters in the saved artifacts.\n",
    "    Consider using torch.save with map_location to specify device.\n",
    "\n",
    "Additional Utility Functions\n",
    "12. flatten_image_list\n",
    "\n",
    "```python\n",
    "\n",
    "def flatten_image_list(image_list):\n",
    "    \"\"\"\n",
    "    Ensure image list is flat in case there are nested lists of image paths.\n",
    "    \"\"\"\n",
    "    if isinstance(image_list, (list, tuple)) and any(isinstance(i, (list, tuple)) for i in image_list):\n",
    "        return [item for sublist in image_list for item in sublist]\n",
    "    return image_list\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    Purpose: Flattens a nested list of image paths into a single list.\n",
    "    Parameters:\n",
    "        image_list: List potentially containing nested lists of image paths.\n",
    "    Returns: Flattened list of image paths.\n",
    "\n",
    "Optimization Suggestion:\n",
    "\n",
    "    Use itertools.chain.from_iterable for potentially faster flattening.\n",
    "\n",
    "13. prepare_unlabeled_data_loader\n",
    "\n",
    "```python\n",
    "\n",
    "def prepare_unlabeled_data_loader(image_files, batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    image_tensors = [transform(Image.open(img)) for img in image_files if os.path.exists(img) and img.endswith('.png')]\n",
    "    if image_tensors:\n",
    "        X_tensor = torch.stack(image_tensors)\n",
    "        dataset = TensorDataset(X_tensor)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return None\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    Purpose: Prepares a DataLoader for unlabeled data (used for self-training).\n",
    "    Parameters:\n",
    "        image_files: List of image file paths.\n",
    "        batch_size: Batch size for DataLoader.\n",
    "    Process:\n",
    "        Applies transformations to images.\n",
    "        Stacks image tensors.\n",
    "        Creates a TensorDataset without labels.\n",
    "        Returns a DataLoader.\n",
    "\n",
    "Optimization Suggestion:\n",
    "\n",
    "    Handle image loading exceptions.\n",
    "    Consider creating a custom Dataset class for better flexibility.\n",
    "\n",
    "Main Function\n",
    "14. main\n",
    "\n",
    "```python\n",
    "\n",
    "def main():\n",
    "    # Define the paths for preprocessed images\n",
    "    lunar_data_images_dir = '../../model/model_output/lunar_preprocessed_images/'\n",
    "    martian_data_images_dir = '../../model/model_output/martian_preprocessed_images/'\n",
    "    save_dir = martian_data_images_dir  # Directory where Martian images are saved\n",
    "\n",
    "    # 1. Load pre-generated lunar images\n",
    "    print(f\"Loading pre-generated lunar data from: {lunar_data_images_dir}\")\n",
    "    lunar_images = preprocess_lunar_data(lunar_data_images_dir)\n",
    "\n",
    "    # Check if any lunar images were loaded\n",
    "    if not lunar_images:\n",
    "        print(\"Error: No lunar images found. Exiting.\")\n",
    "        return  # Exit early if no images found\n",
    "\n",
    "    lunar_labels = [0] * len(lunar_images)  # Placeholder labels for lunar data\n",
    "    lunar_times = [0] * len(lunar_images)   # Placeholder arrival times for lunar data\n",
    "\n",
    "    # Prepare DataLoader for lunar data\n",
    "    print(\"Preparing DataLoader for lunar data...\")\n",
    "    train_loader = prepare_data_for_training(lunar_images, lunar_labels, lunar_times)\n",
    "\n",
    "    if train_loader is None:\n",
    "        print(\"Error: No valid data for training. DataLoader creation failed.\")\n",
    "        return  # Exit early if DataLoader creation failed\n",
    "\n",
    "    # 2. Initialize and train the model on lunar data\n",
    "    print(\"Initializing SpectrogramCNN model...\")\n",
    "    model = SpectrogramCNN()\n",
    "    criterion_event = nn.CrossEntropyLoss()\n",
    "    criterion_time = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print(\"Training model on lunar data...\")\n",
    "    train_model(model, train_loader, criterion_event, criterion_time, optimizer)\n",
    "\n",
    "    # 3. Load pre-generated Martian images\n",
    "    print(f\"Loading pre-generated Martian data from: {martian_data_images_dir}\")\n",
    "    martian_images = load_existing_images(martian_data_images_dir)  # Load images directly\n",
    "\n",
    "    # Check if any Martian images were loaded\n",
    "    if not martian_images:\n",
    "        print(\"Error: No Martian images found. Exiting.\")\n",
    "        return  # Exit early if no images found\n",
    "\n",
    "    # Prepare DataLoader for Martian data\n",
    "    print(\"Preparing DataLoader for Martian data...\")\n",
    "    martian_data_loader = prepare_unlabeled_data_loader(martian_images)\n",
    "\n",
    "    if martian_data_loader is None:\n",
    "        print(\"Error: No valid data for self-training. DataLoader creation failed.\")\n",
    "        return  # Exit early if DataLoader creation failed\n",
    "\n",
    "    # 4. Self-train the model on Martian data\n",
    "    print(\"Self-training model on Martian data...\")\n",
    "    self_train_on_martian_data(model, martian_data_loader, optimizer, criterion_event, criterion_time)\n",
    "\n",
    "    # 5. Save the fine-tuned model\n",
    "    print(\"Saving the fine-tuned model...\")\n",
    "    save_model_artifacts(model, model_name='../../model/fine_tuned_martian_seismic_cnn')\n",
    "```\n",
    "Explanation:\n",
    "\n",
    "    Purpose: Main execution function orchestrating data loading, model training, self-training, and saving.\n",
    "    Process:\n",
    "        Loads lunar and Martian images.\n",
    "        Prepares DataLoaders.\n",
    "        Initializes the model and criteria.\n",
    "        Trains on lunar data.\n",
    "        Self-trains on Martian data.\n",
    "        Saves the fine-tuned model.\n",
    "\n",
    "Optimization Suggestions:\n",
    "\n",
    "    Add exception handling for potential errors.\n",
    "    Validate the sizes of data at each step.\n",
    "    Implement command-line argument parsing for flexibility.\n",
    "\n",
    "Entry Point\n",
    "\n",
    "```python\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "    Purpose: Ensures main() is called when the script is run directly.\n",
    "\n",
    "CNN Analysis and Hyperparameter Tuning Suggestions\n",
    "CNN Architecture Analysis\n",
    "\n",
    "The SpectrogramCNN is designed for processing spectrogram images, which are 2D representations of signal frequencies over time.\n",
    "\n",
    "Architecture Breakdown:\n",
    "\n",
    "    Input Layer:\n",
    "        Accepts grayscale images of size (1, 224, 224) after transformations.\n",
    "\n",
    "    Convolutional Layers:\n",
    "        conv1: 1 input channel, 32 output channels, kernel size 3x3, stride 1, padding 1.\n",
    "        Activation: ReLU.\n",
    "        conv2: 32 input channels, 64 output channels, kernel size 3x3, stride 1, padding 1.\n",
    "        Activation: ReLU.\n",
    "\n",
    "    Pooling Layer:\n",
    "        Max pooling with kernel size 2x2, reducing the spatial dimensions by half.\n",
    "\n",
    "    Fully Connected Layers:\n",
    "        fc1: Input size depends on the output from the convolutional layers (flattened).\n",
    "        fc_event: Outputs logits for 3 classes (event classification).\n",
    "        fc_time: Outputs a single value (time regression).\n",
    "\n",
    "Potential Issues:\n",
    "\n",
    "    Fixed Input Size for fc1:\n",
    "        The input size to fc1 is hardcoded as 64 * 112 * 112, assuming input images are resized to (224, 224) and pooled once.\n",
    "        If the input image size changes, this will cause a dimension mismatch.\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "    Dynamic Calculation of Flattened Size:\n",
    "        Instead of hardcoding, calculate the flattened size based on the input data.\n",
    "\n",
    "    ```python\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SpectrogramCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = None  # Initialize as None\n",
    "        self.fc_event = None\n",
    "        self.fc_time = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(x.size(1), 128).to(x.device)\n",
    "            self.fc_event = nn.Linear(128, 3).to(x.device)\n",
    "            self.fc_time = nn.Linear(128, 1).to(x.device)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc_event(x), self.fc_time(x)\n",
    "```\n",
    "    Batch Normalization and Dropout:\n",
    "        Add nn.BatchNorm2d after convolutional layers to normalize activations.\n",
    "        Add nn.Dropout in the fully connected layers to prevent overfitting.\n",
    "\n",
    "Hyperparameter Tuning Suggestions\n",
    "\n",
    "To improve training on black and white spectrograms, consider the following hyperparameter adjustments:\n",
    "\n",
    "    Learning Rate:\n",
    "        Start with a smaller learning rate, e.g., 1e-4 instead of 1e-3, to ensure stable convergence.\n",
    "\n",
    "    Optimizer:\n",
    "        Experiment with different optimizers like AdamW or SGD with momentum.\n",
    "        Use weight decay to prevent overfitting.\n",
    "\n",
    "    Batch Size:\n",
    "        Adjust batch size based on dataset size and memory constraints.\n",
    "        Smaller batch sizes can lead to noisier gradients but may generalize better.\n",
    "\n",
    "    Number of Epochs:\n",
    "        Increase the number of epochs to allow the model to learn more complex patterns.\n",
    "        Use early stopping based on validation loss to prevent overfitting.\n",
    "\n",
    "    Data Augmentation:\n",
    "        Apply transformations like random rotations, flips, or noise addition to augment the dataset.\n",
    "        This can help the model generalize better to unseen data.\n",
    "\n",
    "    ```python\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    ```\n",
    "    Normalization:\n",
    "        Ensure the normalization parameters (mean and std) are appropriate for grayscale images.\n",
    "        Compute the mean and std from the dataset if possible.\n",
    "\n",
    "    Model Complexity:\n",
    "        Increase the depth of the network by adding more convolutional layers.\n",
    "        Use architectures like ResNet or DenseNet which have shown good performance on image data.\n",
    "\n",
    "    Loss Functions:\n",
    "        For time regression, consider using SmoothL1Loss which is less sensitive to outliers than MSELoss.\n",
    "        For classification, ensure the class imbalance is addressed, possibly by using class weights.\n",
    "\n",
    "    Regularization:\n",
    "        Add L2 regularization through the optimizer's weight decay parameter.\n",
    "        Implement dropout layers to prevent overfitting.\n",
    "\n",
    "    Learning Rate Scheduling:\n",
    "        Use learning rate schedulers like ReduceLROnPlateau to reduce the learning rate when the validation loss plateaus.\n",
    "        Alternatively, use CosineAnnealingLR or StepLR.\n",
    "\n",
    "    Validation Set:\n",
    "        Split a portion of the training data into a validation set to monitor performance during training.\n",
    "\n",
    "    Cross-Validation:\n",
    "        Use k-fold cross-validation to assess model performance more robustly.\n",
    "\n",
    "    Model Initialization:\n",
    "        Use better weight initialization methods, such as Xavier or He initialization.\n",
    "\n",
    "    Transfer Learning:\n",
    "        Utilize pretrained models on similar tasks and fine-tune them on your dataset.\n",
    "\n",
    "    Gradient Clipping:\n",
    "        Apply gradient clipping to prevent exploding gradients.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The codebase provides a framework for training a CNN on spectrogram images for seismic data analysis. By thoroughly analyzing each function and suggesting optimizations, we've identified areas for improvement in data preprocessing, model architecture, and training procedures.\n",
    "\n",
    "Focusing on the CNN, dynamic adjustments to the model's layers and parameters will enhance flexibility and performance. Additionally, careful hyperparameter tuning, considering the specific nature of black and white spectrograms, will improve training outcomes.\n",
    "\n",
    "Implementing the suggested hyperparameter adjustments and optimizations will likely lead to better model performance and generalization on the spectrogram data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
